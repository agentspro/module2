"""
LLM Helper –¥–ª—è RAG –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π
–ü—ñ–¥—Ç—Ä–∏–º–∫–∞ Ollama (–ª–æ–∫–∞–ª—å–Ω–∏–π) —Ç–∞ OpenAI (fallback)
"""
import requests
import json
import os
from typing import List, Dict, Optional


class LLMGenerator:
    """
    –£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é Ollama —Ç–∞ OpenAI
    """

    def __init__(self, prefer_ollama: bool = True):
        """
        Args:
            prefer_ollama: –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ Ollama —Å–ø–æ—á–∞—Ç–∫—É (True), –∞–±–æ —Ç—ñ–ª—å–∫–∏ OpenAI (False)
        """
        self.prefer_ollama = prefer_ollama
        self.ollama_available = self._check_ollama()
        self.openai_available = self._check_openai()

        if self.prefer_ollama and self.ollama_available:
            self.provider = "ollama"
            print("ü§ñ LLM Provider: Ollama (llama3.2:3b)")
        elif self.openai_available:
            self.provider = "openai"
            print("ü§ñ LLM Provider: OpenAI (gpt-4o-mini)")
        else:
            self.provider = "none"
            print("‚ö†Ô∏è  LLM –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π! –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–æ—Å—Ç–∏–π concatenation")

    def _check_ollama(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –¥–æ—Å—Ç—É–ø–Ω–∏–π Ollama"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                models = response.json().get("models", [])
                # –®—É–∫–∞—î–º–æ llama3.2:3b –∞–±–æ –±—É–¥—å-—è–∫—É llama –º–æ–¥–µ–ª—å
                for model in models:
                    if "llama" in model.get("name", "").lower():
                        return True
            return False
        except:
            return False

    def _check_openai(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —î OpenAI API key"""
        return os.getenv("OPENAI_API_KEY") is not None

    def generate_answer(
        self,
        question: str,
        contexts: List[str],
        max_tokens: int = 256
    ) -> str:
        """
        –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤

        Args:
            question: –ó–∞–ø–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            contexts: –°–ø–∏—Å–æ–∫ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤/—á–∞–Ω–∫—ñ–≤
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ

        Returns:
            –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        """
        if self.provider == "ollama":
            return self._generate_with_ollama(question, contexts, max_tokens)
        elif self.provider == "openai":
            return self._generate_with_openai(question, contexts, max_tokens)
        else:
            # Fallback: –ø—Ä–æ—Å—Ç–∏–π concatenation
            return self._simple_concatenation(contexts)

    def _generate_with_ollama(
        self,
        question: str,
        contexts: List[str],
        max_tokens: int
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —á–µ—Ä–µ–∑ Ollama"""
        # –ü–æ–±—É–¥–æ–≤–∞ –ø—Ä–æ–º–ø—Ç—É
        context_text = "\n\n".join([f"[Document {i+1}]\n{ctx}" for i, ctx in enumerate(contexts)])

        prompt = f"""Based on the following documents, please answer the question.
If the answer is not in the documents, say "I don't have enough information to answer this question."

Documents:
{context_text}

Question: {question}

Answer:"""

        try:
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "llama3.2:3b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": max_tokens
                    }
                },
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                print(f"‚ö†Ô∏è  Ollama error: {response.status_code}")
                return self._simple_concatenation(contexts)

        except Exception as e:
            print(f"‚ö†Ô∏è  Ollama exception: {e}")
            return self._simple_concatenation(contexts)

    def _generate_with_openai(
        self,
        question: str,
        contexts: List[str],
        max_tokens: int
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —á–µ—Ä–µ–∑ OpenAI"""
        try:
            from openai import OpenAI
            client = OpenAI()

            context_text = "\n\n".join([f"[Document {i+1}]\n{ctx}" for i, ctx in enumerate(contexts)])

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a helpful assistant that answers questions based on provided documents. If the answer is not in the documents, say so."
                    },
                    {
                        "role": "user",
                        "content": f"""Documents:
{context_text}

Question: {question}

Please provide a concise answer based only on the information in the documents."""
                    }
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            print(f"‚ö†Ô∏è  OpenAI exception: {e}")
            return self._simple_concatenation(contexts)

    def _simple_concatenation(self, contexts: List[str]) -> str:
        """Fallback: –ø—Ä–æ—Å—Ç–∏–π concatenation –±–µ–∑ LLM"""
        if not contexts:
            return "No relevant information found."

        # –û–±–º–µ–∂—É—î–º–æ –∫–æ–∂–µ–Ω context –¥–æ 200 —Å–∏–º–≤–æ–ª—ñ–≤
        truncated = [ctx[:200] + "..." if len(ctx) > 200 else ctx for ctx in contexts[:3]]
        return "\n\n".join(truncated)


# –ì–ª–æ–±–∞–ª—å–Ω–∏–π instance –¥–ª—è –ø–µ—Ä–µ–≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
_llm_generator = None

def get_llm_generator(prefer_ollama: bool = True) -> LLMGenerator:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –≥–ª–æ–±–∞–ª—å–Ω–∏–π LLM generator (singleton pattern)

    Args:
        prefer_ollama: –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ Ollama —Å–ø–æ—á–∞—Ç–∫—É

    Returns:
        LLMGenerator instance
    """
    global _llm_generator
    if _llm_generator is None:
        _llm_generator = LLMGenerator(prefer_ollama=prefer_ollama)
    return _llm_generator
