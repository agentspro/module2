"""
–£—Ç–∏–ª—ñ—Ç–∏ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö
"""
import os
import json
from typing import List, Dict
from pathlib import Path

try:
    import fitz  # PyMuPDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False


class DocumentLoader:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º"""

    def __init__(self, data_dir: str = "data/corporate_docs"):
        self.data_dir = Path(data_dir)

    def load_documents(self, max_documents: int = None) -> List[Dict[str, str]]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ —Ç–µ–∫—Å—Ç–æ–≤—ñ —Ç–∞ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó

        Args:
            max_documents: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (None = –≤—Å—ñ)

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
        """
        documents = []

        if not self.data_dir.exists():
            raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è {self.data_dir} –Ω–µ —ñ—Å–Ω—É—î")

        # –°–ø–æ—á–∞—Ç–∫—É –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Ñ–∞–π–ª–∏
        all_files = list(self.data_dir.glob("*.txt")) + list(self.data_dir.glob("*.pdf"))

        # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if max_documents:
            all_files = all_files[:max_documents]

        for file_path in all_files:
            try:
                if file_path.suffix == '.txt':
                    # –ß–∏—Ç–∞—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—ñ —Ñ–∞–π–ª–∏
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                elif file_path.suffix == '.pdf':
                    # –ß–∏—Ç–∞—î–º–æ PDF —Ñ–∞–π–ª–∏
                    if not HAS_PYMUPDF:
                        print(f"‚ö†Ô∏è  PyMuPDF –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ {file_path.name}")
                        continue

                    doc = fitz.open(file_path)
                    content = ""
                    for page in doc:
                        content += page.get_text()
                    doc.close()
                else:
                    continue

                documents.append({
                    "content": content,
                    "source": file_path.name,
                    "path": str(file_path)
                })
            except Exception as e:
                print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è {file_path.name}: {e}")
                continue

        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
        return documents

    def load_test_queries(self, queries_file: str = "data/test_queries.json") -> Dict:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∏

        Args:
            queries_file: –®–ª—è—Ö –¥–æ JSON —Ñ–∞–π–ª—É –∑ –∑–∞–ø–∏—Ç–∞–º–∏

        Returns:
            Dict: –°–ª–æ–≤–Ω–∏–∫ –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏ –∑–∞–ø–∏—Ç—ñ–≤
        """
        queries_path = Path(queries_file)

        if not queries_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª {queries_file} –Ω–µ —ñ—Å–Ω—É—î")

        with open(queries_path, 'r', encoding='utf-8') as f:
            queries = json.load(f)

        # –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —è–∫ dict —Ç–∞–∫ —ñ list —Ñ–æ—Ä–º–∞—Ç—É
        if isinstance(queries, list):
            # –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ list –≤ dict
            queries_dict = {"general": queries}
            total = len(queries)
        else:
            queries_dict = queries
            total = sum(len(v) for v in queries.values())

        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {total} —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤")
        return queries_dict

    def load_unified_queries(
        self,
        queries_file: str = "data/test_queries_unified.json",
        max_queries: int = None,
        categories: List[str] = None
    ) -> List[Dict]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π —Ç–µ—Å—Ç–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç (100 –∑–∞–ø–∏—Ç—ñ–≤)

        –¶–µ —î–¥–∏–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—Å—ñ—Ö RAG –ø—ñ–¥—Ö–æ–¥—ñ–≤ - –¥–æ–∑–≤–æ–ª—è—î –∫–æ—Ä–µ–∫—Ç–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è!

        Args:
            queries_file: –®–ª—è—Ö –¥–æ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–æ–≥–æ JSON —Ñ–∞–π–ª—É
            max_queries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Ç—ñ–≤ (None = –≤—Å—ñ)
            categories: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó (None = –≤—Å—ñ)

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
        """
        queries_path = Path(queries_file)

        if not queries_path.exists():
            raise FileNotFoundError(
                f"–£–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {queries_file}\n"
                f"–¶–µ–π —Ñ–∞–π–ª –º—ñ—Å—Ç–∏—Ç—å 100 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è RAG –ø—ñ–¥—Ö–æ–¥—ñ–≤."
            )

        with open(queries_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        queries = data.get("queries", [])

        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
        if categories:
            queries = [q for q in queries if q.get("category") in categories]

        # –û–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ
        if max_queries:
            queries = queries[:max_queries]

        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {len(queries)} –∑–∞–ø–∏—Ç—ñ–≤")
        if categories:
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó: {', '.join(categories)}")

        return queries


class TextSplitter:
    """–†–æ–∑–±–∏—Ç—Ç—è —Ç–µ–∫—Å—Ç—É –Ω–∞ —á–∞–Ω–∫–∏"""

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str) -> List[str]:
        """
        –†–æ–∑–±–∏–≤–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∑ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è–º

        Args:
            text: –í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç

        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫—ñ–≤
        """
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + self.chunk_size
            chunk = text[start:end]

            # –ù–∞–º–∞–≥–∞—î–º–æ—Å—å —Ä–æ–∑–±–∏—Ç–∏ –ø–æ —Ä–µ—á–µ–Ω–Ω—è—Ö
            if end < text_length:
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                split_point = max(last_period, last_newline)

                if split_point > self.chunk_size * 0.5:
                    chunk = text[start:start + split_point + 1]
                    end = start + split_point + 1

            chunks.append(chunk.strip())
            start = end - self.chunk_overlap

        return chunks

    def split_documents(self, documents: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        –†–æ–∑–±–∏–≤–∞—î –¥–æ–∫—É–º–µ–Ω—Ç–∏ –Ω–∞ —á–∞–Ω–∫–∏ –∑—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º –º–µ—Ç–∞–¥–∞–Ω–∏—Ö

        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫—ñ–≤ –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
        """
        all_chunks = []

        for doc in documents:
            chunks = self.split_text(doc["content"])

            for i, chunk in enumerate(chunks):
                all_chunks.append({
                    "content": chunk,
                    "source": doc["source"],
                    "chunk_id": i,
                    "total_chunks": len(chunks)
                })

        print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ {len(all_chunks)} —á–∞–Ω–∫—ñ–≤ –∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
        return all_chunks


def save_results(results: Dict, output_file: str):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ—Ü—ñ–Ω–∫–∏ —É —Ñ–∞–π–ª

    Args:
        results: –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        output_file: –®–ª—è—Ö –¥–æ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {output_file}")


def print_results(results: Dict):
    """
    –í–∏–≤–æ–¥–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É —á–∏—Ç–∞–±–µ–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ

    Args:
        results: –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    print("\n" + "="*60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò –û–¶–Ü–ù–ö–ò")
    print("="*60)

    if "system_name" in results:
        print(f"\nüîß –°–∏—Å—Ç–µ–º–∞: {results['system_name']}")

    if "metrics" in results:
        print(f"\nüìà –ú–µ—Ç—Ä–∏–∫–∏:")
        for metric, value in results["metrics"].items():
            if isinstance(value, float):
                print(f"  {metric}: {value:.3f}")
            else:
                print(f"  {metric}: {value}")

    if "execution_time" in results:
        print(f"\n‚è±Ô∏è  –ß–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {results['execution_time']:.2f} —Å–µ–∫—É–Ω–¥")

    print("\n" + "="*60)
