[
  {
    "question": "What is Retrieval-Augmented Generation (RAG) and how does it work?",
    "ground_truth": "RAG is a technique that combines information retrieval with text generation. It works by first retrieving relevant documents from a knowledge base using semantic search, then using these documents as context for a language model to generate accurate, grounded responses. This approach reduces hallucinations and provides factual answers based on retrieved evidence.",
    "category": "definition",
    "expected_context": ["retrieval", "generation", "knowledge base", "documents", "context"]
  },
  {
    "question": "Explain the main components of a RAG system",
    "ground_truth": "A RAG system consists of three main components: (1) Document Store - a database or vector store containing the knowledge base, (2) Retriever - typically using embeddings and semantic search to find relevant documents, and (3) Generator - a language model that uses retrieved documents as context to produce answers. The retriever and generator work together to provide accurate, contextual responses.",
    "category": "definition",
    "expected_context": ["document store", "retriever", "generator", "embeddings", "vector store"]
  },
  {
    "question": "What are the key advantages of using RAG over traditional language models?",
    "ground_truth": "RAG offers several advantages: (1) Reduced hallucinations by grounding responses in retrieved documents, (2) Up-to-date information without retraining the model, (3) Transparency through source attribution, (4) Domain adaptation by simply updating the document store, and (5) Better factual accuracy by leveraging external knowledge bases rather than relying solely on parametric memory.",
    "category": "definition",
    "expected_context": ["hallucinations", "factual accuracy", "transparency", "source attribution", "external knowledge"]
  },
  {
    "question": "How does the retrieval mechanism work in RAG systems?",
    "ground_truth": "The retrieval mechanism in RAG typically uses dense vector embeddings to represent both queries and documents in a shared semantic space. When a query arrives, it is encoded into an embedding vector, and similarity search (often cosine similarity) is performed against the document embeddings to find the most relevant chunks. The top-k most similar documents are retrieved and passed to the generator as context.",
    "category": "technical",
    "expected_context": ["embeddings", "vector", "semantic space", "similarity search", "cosine similarity"]
  },
  {
    "question": "What is the role of the retriever in RAG architecture?",
    "ground_truth": "The retriever is responsible for finding and ranking relevant documents from the knowledge base given a user query. It converts the query into a representation (typically embeddings), searches the document store, and returns the most relevant documents based on similarity scores. The quality of retrieval directly impacts the final answer quality, making the retriever a critical component.",
    "category": "technical",
    "expected_context": ["retriever", "ranking", "similarity", "knowledge base", "relevance"]
  },
  {
    "question": "Explain the difference between dense and sparse retrievers in RAG",
    "ground_truth": "Dense retrievers use neural embeddings to represent queries and documents as continuous vectors in semantic space, capturing meaning and context. They excel at semantic matching but require training. Sparse retrievers like BM25 use discrete term-matching with TF-IDF weighting, focusing on exact keyword overlap. They are fast, interpretable, and require no training. Hybrid approaches combine both for optimal performance.",
    "category": "technical",
    "expected_context": ["dense retrieval", "sparse retrieval", "BM25", "embeddings", "semantic", "keywords"]
  },
  {
    "question": "How are embeddings used in RAG systems?",
    "ground_truth": "Embeddings are vector representations that encode semantic meaning of text. In RAG, documents are chunked and converted into embeddings during indexing, stored in a vector database. At query time, the user question is also embedded using the same model. Cosine similarity or other distance metrics are used to find documents whose embeddings are closest to the query embedding, enabling semantic search beyond keyword matching.",
    "category": "technical",
    "expected_context": ["embeddings", "vector", "semantic meaning", "vector database", "similarity"]
  },
  {
    "question": "What is the purpose of reranking in advanced RAG?",
    "ground_truth": "Reranking is a second-stage refinement that improves initial retrieval results. After the first-pass retrieval returns candidate documents, a more sophisticated reranker (often a cross-encoder model) scores each document-query pair more accurately. This two-stage approach balances efficiency (fast first-pass retrieval) with quality (accurate reranking), significantly improving the relevance of documents sent to the generator.",
    "category": "technical",
    "expected_context": ["reranking", "cross-encoder", "relevance", "two-stage", "refinement"]
  },
  {
    "question": "What is Self-RAG and how does it differ from standard RAG?",
    "ground_truth": "Self-RAG extends standard RAG by adding self-reflection capabilities through special reflection tokens. It can decide when retrieval is needed, evaluate retrieval quality, and critique its own generated answers. Self-RAG uses multiple reflection tokens (IsREL, IsSUP, IsUSE) to assess whether retrieved documents are relevant, whether the answer is supported by evidence, and whether the answer is useful. This allows iterative refinement and adaptive retrieval.",
    "category": "approaches",
    "expected_context": ["Self-RAG", "self-reflection", "reflection tokens", "adaptive", "iterative"]
  },
  {
    "question": "Explain the concept of Corrective RAG (CRAG)",
    "ground_truth": "Corrective RAG (CRAG) improves retrieval quality through a correction mechanism. It evaluates retrieved documents and classifies them as correct, ambiguous, or incorrect. For correct documents, it uses knowledge refinement to extract key information. For ambiguous cases, it combines refined knowledge with web search. For incorrect results, it discards retrieval and relies on web search or query rewriting. This corrective approach significantly reduces errors from poor retrieval.",
    "category": "approaches",
    "expected_context": ["CRAG", "correction", "evaluation", "web search", "knowledge refinement"]
  },
  {
    "question": "What is REPLUG and how does it improve RAG performance?",
    "ground_truth": "REPLUG (Retrieve and Plug) is a retrieval-augmented approach that treats the language model as a black box. It retrieves multiple documents for each input, prepends them as context, and ensembles predictions across different retrieved document sets. REPLUG can work with frozen language models and improves performance through better document selection and ensemble techniques, making it particularly useful for adapting existing models without retraining.",
    "category": "approaches",
    "expected_context": ["REPLUG", "ensemble", "black box", "frozen models", "document selection"]
  },
  {
    "question": "How does Atlas approach retrieval-augmented generation?",
    "ground_truth": "Atlas is a retrieval-augmented model that jointly trains the retriever and generator end-to-end. Unlike standard RAG which uses a frozen retriever, Atlas optimizes both components together, allowing the retriever to learn what documents are most useful for generation. It uses a dense retriever based on Contriever and a seq2seq generator, achieving strong performance on knowledge-intensive tasks through this joint training approach.",
    "category": "approaches",
    "expected_context": ["Atlas", "joint training", "end-to-end", "Contriever", "knowledge-intensive"]
  },
  {
    "question": "What are the main differences between naive RAG and advanced RAG?",
    "ground_truth": "Naive RAG uses simple retrieval and generation: embed query, retrieve top-k documents, generate answer. Advanced RAG adds multiple improvements: query rewriting for better retrieval, hybrid search combining semantic and keyword methods, reranking for quality, metadata filtering, and iterative refinement. Advanced RAG also includes query analysis, context compression, and answer verification, resulting in significantly higher accuracy (85-90%) compared to naive RAG (30-50%).",
    "category": "approaches",
    "expected_context": ["naive RAG", "advanced RAG", "query rewriting", "hybrid search", "reranking"]
  },
  {
    "question": "How can we evaluate the quality of RAG systems?",
    "ground_truth": "RAG systems can be evaluated using multiple metrics: (1) Retrieval metrics - precision, recall, MRR, NDCG for document relevance, (2) Generation metrics - BLEU, ROUGE for text quality, (3) End-to-end metrics - exact match, F1 score for answer accuracy, and (4) LLM-based metrics like RAGAS (faithfulness, answer relevancy, context precision/recall). Human evaluation of correctness, relevance, and usefulness is also important.",
    "category": "evaluation",
    "expected_context": ["evaluation", "metrics", "precision", "recall", "RAGAS", "faithfulness"]
  },
  {
    "question": "What metrics are commonly used to assess RAG performance?",
    "ground_truth": "Common RAG metrics include: Retrieval - MRR (Mean Reciprocal Rank), NDCG (Normalized Discounted Cumulative Gain), Recall@k. Generation - BLEU, ROUGE, BERTScore. RAG-specific - Faithfulness (answer grounded in context), Answer Relevancy (relevance to question), Context Precision (relevant docs in top-k), Context Recall (all relevant docs retrieved). Latency and cost are also important production metrics.",
    "category": "evaluation",
    "expected_context": ["MRR", "NDCG", "faithfulness", "relevancy", "precision", "recall"]
  },
  {
    "question": "What is faithfulness in RAG evaluation?",
    "ground_truth": "Faithfulness measures whether the generated answer is factually grounded in the retrieved context. It evaluates if claims in the answer can be traced back to and supported by the source documents. High faithfulness means the model is not hallucinating or adding information beyond what's in the retrieved context. This is assessed by checking if each statement in the answer has supporting evidence in the provided documents.",
    "category": "evaluation",
    "expected_context": ["faithfulness", "grounded", "hallucination", "evidence", "factual"]
  },
  {
    "question": "How do we measure context precision and recall in RAG?",
    "ground_truth": "Context Precision measures what fraction of retrieved documents are actually relevant to answering the question - high precision means few irrelevant documents. Context Recall measures whether all relevant documents were retrieved - high recall means no important information was missed. Together, they assess retrieval quality: precision ensures efficiency (no noise), recall ensures completeness (no gaps). F1 score combines both metrics.",
    "category": "evaluation",
    "expected_context": ["precision", "recall", "relevant documents", "retrieval quality", "F1"]
  },
  {
    "question": "What are the main challenges in implementing RAG systems?",
    "ground_truth": "Main challenges include: (1) Chunking strategy - balancing context size vs relevance, (2) Retrieval quality - handling ambiguous queries and ensuring relevant documents are found, (3) Context length limits - fitting multiple documents within model constraints, (4) Latency - retrieval and generation add overhead, (5) Cost - embedding models and API calls, (6) Evaluation - measuring real-world usefulness, and (7) Keeping knowledge base updated and handling contradictory information.",
    "category": "practical",
    "expected_context": ["chunking", "retrieval quality", "latency", "context length", "evaluation"]
  },
  {
    "question": "How can we optimize retrieval performance in production RAG systems?",
    "ground_truth": "Optimization strategies include: (1) Use approximate nearest neighbor search (FAISS, HNSW) for speed, (2) Implement caching for frequent queries, (3) Optimize chunk size (500-1000 tokens) and overlap (10-20%), (4) Use hybrid search combining dense and sparse retrieval, (5) Employ metadata filtering to narrow search space, (6) Batch embedding generation, (7) Use reranking selectively on top results, and (8) Monitor and tune top-k parameter based on accuracy-latency tradeoff.",
    "category": "practical",
    "expected_context": ["optimization", "FAISS", "caching", "hybrid search", "chunk size", "reranking"]
  },
  {
    "question": "What are the trade-offs between different RAG approaches?",
    "ground_truth": "Key trade-offs: Naive RAG is fast (2-3s) but low accuracy (30-50%). Advanced RAG balances accuracy (85-90%) and speed (3-4s) - best for production. Self-RAG and CRAG achieve highest accuracy (90-95%) but slower (4-5s) and more complex. Graph RAG excels at relationship queries but requires graph construction overhead. Agentic RAG handles complex reasoning but has highest latency. Trade-offs involve accuracy vs speed, simplicity vs capability, and cost vs quality.",
    "category": "practical",
    "expected_context": ["trade-offs", "accuracy", "latency", "complexity", "cost", "production"]
  },
  {
    "question": "How does query rewriting improve RAG accuracy?",
    "ground_truth": "Query rewriting transforms user queries into better retrieval queries. It expands abbreviations, adds context, resolves ambiguity, and reformulates questions for better semantic matching. For example, 'How does CRAG work?' becomes 'Explain the mechanism and components of Corrective Retrieval-Augmented Generation including document evaluation and knowledge refinement'. This improves retrieval recall by ensuring embeddings capture full intent and increasing overlap with document content.",
    "category": "practical",
    "expected_context": ["query rewriting", "reformulation", "semantic matching", "retrieval recall", "ambiguity"]
  },
  {
    "question": "What is hybrid search and why is it effective in RAG?",
    "ground_truth": "Hybrid search combines dense retrieval (semantic embeddings) with sparse retrieval (keyword-based like BM25). Dense retrieval captures meaning and handles synonyms, while sparse retrieval ensures exact term matches and handles rare/technical terms. Results are merged using techniques like Reciprocal Rank Fusion (RRF). This combination leverages strengths of both approaches: semantic understanding from dense retrieval and precision from sparse retrieval, typically improving accuracy by 15-25%.",
    "category": "practical",
    "expected_context": ["hybrid search", "dense retrieval", "sparse retrieval", "BM25", "RRF", "semantic"]
  },
  {
    "question": "What is the role of chunking in RAG systems?",
    "ground_truth": "Chunking splits documents into smaller, manageable pieces for efficient retrieval and generation. Optimal chunk size balances context completeness with precision - too small loses context, too large adds noise. Typical sizes are 500-1000 characters with 10-20% overlap to maintain continuity. Good chunking respects semantic boundaries (paragraphs, sentences), preserves context through overlap, and includes metadata for filtering. Poor chunking significantly degrades RAG performance.",
    "category": "practical",
    "expected_context": ["chunking", "chunk size", "overlap", "semantic boundaries", "context"]
  }
]
