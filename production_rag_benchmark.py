"""
Production RAG Benchmark - –†–µ–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î sentence-transformers –¥–ª—è embeddings —Ç–∞ –≤–∏–º—ñ—Ä—é—î —Ä–µ–∞–ª—å–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
"""
import sys
import time
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict

# –°–ø—Ä–æ–±–∞ —ñ–º–ø–æ—Ä—Ç—É production –±—ñ–±–ª—ñ–æ—Ç–µ–∫
try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
    print("‚úÖ sentence-transformers –¥–æ—Å—Ç—É–ø–Ω–∏–π")
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("‚ö†Ô∏è  sentence-transformers –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é TF-IDF")

from utils.data_loader import DocumentLoader, TextSplitter


class ProductionEmbeddings:
    """Production-ready embeddings –∑ sentence-transformers –∞–±–æ TF-IDF fallback"""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name

        if HAS_SENTENCE_TRANSFORMERS:
            print(f"üîß –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ {model_name}...")
            start = time.time()
            self.model = SentenceTransformer(model_name)
            load_time = time.time() - start
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}—Å")
            self.mode = "transformer"
        else:
            print("üìä –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é TF-IDF (fallback)...")
            self.vocabulary = {}
            self.idf = {}
            self.mode = "tfidf"

    def fit(self, texts: List[str]):
        """–ù–∞–≤—á–∞–Ω–Ω—è (—Ç—ñ–ª—å–∫–∏ –¥–ª—è TF-IDF)"""
        if self.mode == "tfidf":
            doc_word_sets = []
            for doc in texts:
                words = set(doc.lower().split())
                doc_word_sets.append(words)
                for word in words:
                    self.vocabulary[word] = self.vocabulary.get(word, 0) + 1

            num_docs = len(texts)
            for word in self.vocabulary:
                doc_count = sum(1 for word_set in doc_word_sets if word in word_set)
                self.idf[word] = np.log(num_docs / (doc_count + 1)) + 1

    def embed_batch(self, texts: List[str], show_progress: bool = False) -> np.ndarray:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è embeddings –¥–ª—è batch —Ç–µ–∫—Å—Ç—ñ–≤"""
        if self.mode == "transformer":
            return self.model.encode(texts, show_progress_bar=show_progress, convert_to_numpy=True)
        else:
            return np.array([self._embed_tfidf(text) for text in texts])

    def embed(self, text: str) -> np.ndarray:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è embedding –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É"""
        if self.mode == "transformer":
            return self.model.encode([text], convert_to_numpy=True)[0]
        else:
            return self._embed_tfidf(text)

    def _embed_tfidf(self, text: str) -> np.ndarray:
        """TF-IDF embedding (fallback)"""
        words = text.lower().split()
        word_count = defaultdict(int)
        for word in words:
            word_count[word] += 1

        vector = np.zeros(len(self.vocabulary))
        for i, word in enumerate(sorted(self.vocabulary.keys())):
            if word in word_count:
                tf = word_count[word] / max(len(words), 1)
                idf = self.idf.get(word, 1)
                vector[i] = tf * idf

        return vector

    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–∞ –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å"""
        dot = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return dot / (norm1 * norm2 + 1e-10)


class ProductionRAGBenchmark:
    """Benchmark —Ä—ñ–∑–Ω–∏—Ö RAG –ø—ñ–¥—Ö–æ–¥—ñ–≤ –∑ —Ä–µ–∞–ª—å–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""

    def __init__(self, embedding_model: str = "all-MiniLM-L6-v2"):
        self.embedding_model = ProductionEmbeddings(embedding_model)
        self.chunks = []
        self.chunk_embeddings = None
        self.documents = []

    def load_data(self, docs_path: str = "data/corporate_docs"):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö"""
        print(f"\nüìö –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ {docs_path}...")

        loader = DocumentLoader(docs_path)
        self.documents = loader.load_documents()

        splitter = TextSplitter(chunk_size=500, chunk_overlap=100)
        self.chunks = splitter.split_documents(self.documents)

        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ -> {len(self.chunks)} —á–∞–Ω–∫—ñ–≤")

    def create_embeddings(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è embeddings –∑ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è–º —á–∞—Å—É"""
        print(f"\nüî¢ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è embeddings –¥–ª—è {len(self.chunks)} —á–∞–Ω–∫—ñ–≤...")

        texts = [chunk["content"] for chunk in self.chunks]

        # –ù–∞–≤—á–∞–Ω–Ω—è (–¥–ª—è TF-IDF)
        if self.embedding_model.mode == "tfidf":
            self.embedding_model.fit(texts)

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è embeddings –∑ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è–º —á–∞—Å—É
        start = time.time()
        self.chunk_embeddings = self.embedding_model.embed_batch(texts, show_progress=True)
        embedding_time = time.time() - start

        print(f"‚úÖ Embeddings —Å—Ç–≤–æ—Ä–µ–Ω–æ –∑–∞ {embedding_time:.2f}—Å")
        print(f"   –®–≤–∏–¥–∫—ñ—Å—Ç—å: {len(self.chunks) / embedding_time:.1f} docs/sec")
        print(f"   –†–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä—É: {self.chunk_embeddings.shape[1]}")

        return {
            "embedding_time": embedding_time,
            "docs_per_second": len(self.chunks) / embedding_time,
            "vector_dimension": self.chunk_embeddings.shape[1],
            "model": self.embedding_model.model_name,
            "mode": self.embedding_model.mode
        }

    def benchmark_retrieval(self, queries: List[str], top_k: int = 5) -> Dict:
        """Benchmark retrieval –∑ –¥–µ—Ç–∞–ª—å–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        print(f"\nüîç Benchmark retrieval –¥–ª—è {len(queries)} –∑–∞–ø–∏—Ç—ñ–≤...")

        results = []
        retrieval_times = []

        for query in queries:
            start = time.time()

            # –°—Ç–≤–æ—Ä—é—î–º–æ embedding –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self.embedding_model.embed(query)

            # –†–∞—Ö—É—î–º–æ –ø–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –∑ —É—Å—ñ–º–∞ —á–∞–Ω–∫–∞–º–∏
            similarities = []
            for i, chunk_embedding in enumerate(self.chunk_embeddings):
                similarity = self.embedding_model.cosine_similarity(query_embedding, chunk_embedding)
                similarities.append((i, similarity))

            # –¢–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            similarities.sort(key=lambda x: x[1], reverse=True)
            top_results = similarities[:top_k]

            retrieval_time = time.time() - start
            retrieval_times.append(retrieval_time)

            results.append({
                "query": query,
                "top_k_scores": [score for _, score in top_results],
                "top_k_sources": [self.chunks[idx]["source"] for idx, _ in top_results],
                "retrieval_time": retrieval_time
            })

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        avg_time = np.mean(retrieval_times)
        p50 = np.percentile(retrieval_times, 50)
        p95 = np.percentile(retrieval_times, 95)
        p99 = np.percentile(retrieval_times, 99)

        metrics = {
            "total_queries": len(queries),
            "avg_retrieval_time": avg_time,
            "p50_latency": p50,
            "p95_latency": p95,
            "p99_latency": p99,
            "queries_per_second": 1 / avg_time if avg_time > 0 else 0,
            "avg_top1_score": np.mean([r["top_k_scores"][0] for r in results]),
            "avg_top5_score": np.mean([np.mean(r["top_k_scores"]) for r in results]),
            "results": results
        }

        print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ retrieval:")
        print(f"   –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å: {avg_time*1000:.1f}ms")
        print(f"   P50 latency: {p50*1000:.1f}ms")
        print(f"   P95 latency: {p95*1000:.1f}ms")
        print(f"   P99 latency: {p99*1000:.1f}ms")
        print(f"   QPS: {metrics['queries_per_second']:.1f} queries/sec")
        print(f"   –°–µ—Ä–µ–¥–Ω—ñ–π top-1 score: {metrics['avg_top1_score']:.3f}")

        return metrics

    def benchmark_end_to_end(self, queries: List[str], expected_answers: List[str] = None) -> Dict:
        """End-to-end benchmark –∑ accuracy –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        print(f"\nüéØ End-to-end benchmark...")

        e2e_times = []
        accuracy_scores = []

        for i, query in enumerate(queries):
            start = time.time()

            # Retrieval
            query_embedding = self.embedding_model.embed(query)
            similarities = [(j, self.embedding_model.cosine_similarity(query_embedding, emb))
                          for j, emb in enumerate(self.chunk_embeddings)]
            similarities.sort(key=lambda x: x[1], reverse=True)

            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ (—Å–∏–º—É–ª—é—î–º–æ)
            best_chunk = self.chunks[similarities[0][0]]
            answer = best_chunk["content"][:200]

            e2e_time = time.time() - start
            e2e_times.append(e2e_time)

            # Accuracy (—è–∫—â–æ —î ground truth)
            if expected_answers and i < len(expected_answers):
                # –ü—Ä–æ—Å—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞: —á–∏ —î –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤ –∑–Ω–∞–π–¥–µ–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ
                expected_words = set(expected_answers[i].lower().split())
                found_words = set(answer.lower().split())
                overlap = len(expected_words & found_words) / len(expected_words) if expected_words else 0
                accuracy_scores.append(overlap)

        metrics = {
            "avg_e2e_time": np.mean(e2e_times),
            "total_time": sum(e2e_times),
            "avg_accuracy": np.mean(accuracy_scores) if accuracy_scores else None
        }

        print(f"\nüìà End-to-end –º–µ—Ç—Ä–∏–∫–∏:")
        print(f"   –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å: {metrics['avg_e2e_time']*1000:.1f}ms")
        print(f"   –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {metrics['total_time']:.2f}s")
        if metrics['avg_accuracy']:
            print(f"   –°–µ—Ä–µ–¥–Ω—è accuracy: {metrics['avg_accuracy']:.1%}")

        return metrics


def run_production_benchmark():
    """–ó–∞–ø—É—Å–∫ –ø–æ–≤–Ω–æ–≥–æ production benchmark"""
    print("="*70)
    print("üöÄ PRODUCTION RAG BENCHMARK")
    print("="*70)

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    benchmark = ProductionRAGBenchmark()

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    benchmark.load_data()

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è embeddings
    embedding_metrics = benchmark.create_embeddings()

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
    loader = DocumentLoader()
    test_queries = loader.load_test_queries()

    # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –∑–∞–ø–∏—Ç–∏
    all_queries = []
    expected_answers = []
    for category in ["simple_queries", "medium_queries", "complex_queries"]:
        for q in test_queries[category]:
            all_queries.append(q["question"])
            expected_answers.append(q["expected_answer"])

    # Benchmark retrieval
    retrieval_metrics = benchmark.benchmark_retrieval(all_queries, top_k=5)

    # End-to-end benchmark
    e2e_metrics = benchmark.benchmark_end_to_end(all_queries, expected_answers)

    # –ü—ñ–¥—Å—É–º–∫–æ–≤–∏–π –∑–≤—ñ—Ç
    final_report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "system": {
            "embedding_model": benchmark.embedding_model.model_name,
            "embedding_mode": benchmark.embedding_model.mode,
            "documents": len(benchmark.documents),
            "chunks": len(benchmark.chunks),
            "vector_dimension": embedding_metrics["vector_dimension"]
        },
        "embedding_performance": embedding_metrics,
        "retrieval_performance": {
            k: v for k, v in retrieval_metrics.items() if k != "results"
        },
        "end_to_end_performance": e2e_metrics,
        "sample_results": retrieval_metrics["results"][:3]  # –ü–µ—Ä—à—ñ 3 –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É
    }

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
    output_file = "results/production_benchmark.json"
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)

    print(f"\nüíæ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {output_file}")

    # –í–∏–≤–µ–¥–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫—É
    print(f"\n{'='*70}")
    print("üìä –ü–Ü–î–°–£–ú–û–ö BENCHMARK")
    print(f"{'='*70}")
    print(f"\nüîß –°–∏—Å—Ç–µ–º–∞:")
    print(f"   –ú–æ–¥–µ–ª—å: {benchmark.embedding_model.model_name} ({benchmark.embedding_model.mode})")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {len(benchmark.documents)}")
    print(f"   –ß–∞–Ω–∫—ñ–≤: {len(benchmark.chunks)}")
    print(f"   –†–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä—É: {embedding_metrics['vector_dimension']}")

    print(f"\n‚ö° –ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å Embeddings:")
    print(f"   –ß–∞—Å —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è: {embedding_metrics['embedding_time']:.2f}s")
    print(f"   –®–≤–∏–¥–∫—ñ—Å—Ç—å: {embedding_metrics['docs_per_second']:.1f} docs/sec")

    print(f"\nüîç –ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å Retrieval:")
    print(f"   –°–µ—Ä–µ–¥–Ω—è latency: {retrieval_metrics['avg_retrieval_time']*1000:.1f}ms")
    print(f"   P95 latency: {retrieval_metrics['p95_latency']*1000:.1f}ms")
    print(f"   QPS: {retrieval_metrics['queries_per_second']:.1f}")
    print(f"   Accuracy (top-1): {retrieval_metrics['avg_top1_score']:.1%}")

    print(f"\nüéØ End-to-End:")
    print(f"   –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å: {e2e_metrics['avg_e2e_time']*1000:.1f}ms")
    if e2e_metrics['avg_accuracy']:
        print(f"   –°–µ—Ä–µ–¥–Ω—è accuracy: {e2e_metrics['avg_accuracy']:.1%}")

    print(f"\n‚úÖ Production benchmark –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    return final_report


if __name__ == "__main__":
    report = run_production_benchmark()
