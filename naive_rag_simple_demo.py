#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∏–π Naive RAG Demo –¥–ª—è –≤–æ—Ä–∫—à–æ–ø—É
–ù–µ –ø–æ—Ç—Ä–µ–±—É—î –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö —Ñ–∞–π–ª—ñ–≤ - –≤—Å–µ –≤–±—É–¥–æ–≤–∞–Ω–æ
"""

import time
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –í–±—É–¥–æ–≤–∞–Ω—ñ —Ç–µ—Å—Ç–æ–≤—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏
SAMPLE_DOCUMENTS = [
    "Retrieval-Augmented Generation (RAG) combines information retrieval with text generation. It was introduced by Facebook AI Research in 2020.",
    "RAG uses a retriever to fetch relevant documents from a knowledge base, then passes them to a generator model to produce answers.",
    "Dense retrieval methods use neural embeddings to represent queries and documents in a continuous vector space.",
    "Sparse retrieval methods like TF-IDF and BM25 rely on exact keyword matching and term frequency statistics.",
    "Cross-encoders provide better accuracy for reranking but are slower than bi-encoders because they process query and document together.",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search in high-dimensional spaces.",
    "Transformers use self-attention mechanisms to process sequences in parallel, unlike RNNs which process sequentially.",
    "BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP by using bidirectional context.",
    "Fine-tuning adapts a pre-trained model to a specific task by training on task-specific data.",
    "Embeddings map discrete tokens to continuous vectors, capturing semantic meaning and relationships between words."
]

# –¢–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∏
TEST_QUERIES = [
    "What is retrieval-augmented generation?",
    "Explain the difference between dense and sparse retrieval",
    "How does self-attention work in transformers?"
]


class NaiveRAG:
    """–ù–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∞ RAG —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è"""
    
    def __init__(self, documents):
        self.documents = documents
        self.vectorizer = None
        self.tfidf_matrix = None
        
    def build_index(self):
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ TF-IDF —ñ–Ω–¥–µ–∫—Å"""
        print("üî¢ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è TF-IDF —ñ–Ω–¥–µ–∫—Å—É...")
        start = time.time()
        
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        
        self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)
        
        elapsed = time.time() - start
        print(f"‚úÖ –Ü–Ω–¥–µ–∫—Å —Å—Ç–≤–æ—Ä–µ–Ω–æ –∑–∞ {elapsed*1000:.1f}ms")
        print(f"   –°–ª–æ–≤–Ω–∏–∫: {len(self.vectorizer.vocabulary_)} —Å–ª—ñ–≤")
        print(f"   –î–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {len(self.documents)}")
        
    def retrieve(self, query, top_k=3):
        """–ó–Ω–∞–π—Ç–∏ top-k –Ω–∞–π–±—ñ–ª—å—à —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤"""
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]
        
        # –¢–æ–ø-k —ñ–Ω–¥–µ–∫—Å–∏
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'doc_id': idx,
                'score': similarities[idx],
                'text': self.documents[idx]
            })
        
        return results
    
    def query(self, question, top_k=3):
        """–í–∏–∫–æ–Ω–∞—Ç–∏ RAG –∑–∞–ø–∏—Ç"""
        print(f"\n{'='*60}")
        print(f"‚ùì Query: {question}")
        print(f"{'='*60}")
        
        # –ö—Ä–æ–∫ 1: Retrieval
        start_retrieval = time.time()
        results = self.retrieve(question, top_k=top_k)
        retrieval_time = time.time() - start_retrieval
        
        print(f"\nüìö Retrieved {len(results)} documents ({retrieval_time*1000:.1f}ms):")
        for i, result in enumerate(results, 1):
            print(f"\n  {i}. [Score: {result['score']:.4f}]")
            print(f"     {result['text'][:100]}...")
        
        # –ö—Ä–æ–∫ 2: Generation (—Å–∏–º—É–ª—é—î–º–æ - –≤ production –±—É–≤ –±–∏ LLM)
        print(f"\nüí≠ Generating answer (simulated)...")
        context = "\n".join([r['text'] for r in results])
        
        # –ü—Ä–æ—Å—Ç–µ summary –Ω–∞ –æ—Å–Ω–æ–≤—ñ retrieved docs
        answer = f"Based on the retrieved documents: {results[0]['text']}"
        
        print(f"\n‚úÖ Answer:")
        print(f"   {answer[:200]}...")
        
        return {
            'question': question,
            'answer': answer,
            'retrieved_docs': results,
            'retrieval_time': retrieval_time,
            'top_score': results[0]['score'] if results else 0
        }


def run_demo():
    """–ó–∞–ø—É—Å—Ç–∏—Ç–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—é"""
    print("="*60)
    print("üöÄ NAIVE RAG DEMO")
    print("="*60)
    print()
    
    # –ö—Ä–æ–∫ 1: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    print(f"üìÑ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ {len(SAMPLE_DOCUMENTS)} –≤–±—É–¥–æ–≤–∞–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
    rag = NaiveRAG(SAMPLE_DOCUMENTS)
    
    # –ö—Ä–æ–∫ 2: –ü–æ–±—É–¥—É–≤–∞—Ç–∏ —ñ–Ω–¥–µ–∫—Å
    rag.build_index()
    
    # –ö—Ä–æ–∫ 3: –¢–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∏
    print(f"\nüéØ –ó–∞–ø—É—Å–∫–∞—î–º–æ {len(TEST_QUERIES)} —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤...")
    
    results = []
    for query in TEST_QUERIES:
        result = rag.query(query, top_k=3)
        results.append(result)
        time.sleep(0.5)  # –ü–∞—É–∑–∞ –¥–ª—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ
    
    # –ö—Ä–æ–∫ 4: –ü—ñ–¥—Å—É–º–æ–∫
    print(f"\n{'='*60}")
    print("üìä –ü–Ü–î–°–£–ú–û–ö")
    print(f"{'='*60}")
    
    avg_retrieval_time = np.mean([r['retrieval_time'] for r in results])
    avg_score = np.mean([r['top_score'] for r in results])
    
    print(f"\n‚úÖ –í–∏–∫–æ–Ω–∞–Ω–æ {len(results)} –∑–∞–ø–∏—Ç—ñ–≤")
    print(f"‚è±Ô∏è  –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å retrieval: {avg_retrieval_time*1000:.1f}ms")
    print(f"üìà –°–µ—Ä–µ–¥–Ω—ñ–π top-1 score: {avg_score:.4f}")
    
    # –ö–ª—é—á–æ–≤—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è
    print(f"\n‚ö†Ô∏è  –û–ë–ú–ï–ñ–ï–ù–ù–Ø NAIVE RAG:")
    print(f"   ‚ùå Keyword-only search (–Ω–µ–º–∞—î —Å–µ–º–∞–Ω—Ç–∏–∫–∏)")
    print(f"   ‚ùå –ù–µ–º–∞—î reranking (–ø–µ—Ä—à—ñ results –Ω–µ –∑–∞–≤–∂–¥–∏ –Ω–∞–π–∫—Ä–∞—â—ñ)")
    print(f"   ‚ùå TF-IDF –Ω–µ —Ä–æ–∑—É–º—ñ—î —Å–∏–Ω–æ–Ω—ñ–º–∏")
    print(f"   ‚ùå –ù–∏–∑—å–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å (~30%)")
    print(f"\n   ‚úÖ –®–≤–∏–¥–∫—ñ—Å—Ç—å: —Ö–æ—Ä–æ—à–∞ (~{avg_retrieval_time*1000:.0f}ms)")
    print(f"   ‚úÖ –ü—Ä–æ—Å—Ç–æ—Ç–∞: –¥—É–∂–µ –ø—Ä–æ—Å—Ç–∞ —ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è")
    
    print(f"\nüí° –î–õ–Ø PRODUCTION:")
    print(f"   ‚Üí –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ Retrieve-and-Rerank")
    print(f"   ‚Üí Dense embeddings (FAISS)")
    print(f"   ‚Üí Cross-encoder reranking")
    print(f"   ‚Üí Accuracy –¥–æ 85-90%")
    
    print(f"\n{'='*60}")


if __name__ == "__main__":
    run_demo()
