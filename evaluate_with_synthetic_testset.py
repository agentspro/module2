"""
–ü—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π RAG Evaluation –∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
===================================================

–û—Ü—ñ–Ω—é—î RAG —Å–∏—Å—Ç–µ–º—É –Ω–∞ 50-200+ —Ä–µ–∞–ª—å–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤–∏—Ö –∫–µ–π—Å–∞—Ö –∑ —É—Å—ñ–º–∞ RAGAS –º–µ—Ç—Ä–∏–∫–∞–º–∏:

RAGAS Metrics (4 –æ—Å–Ω–æ–≤–Ω—ñ):
1. Faithfulness (0-1)         - —á–∏ –±–∞–∑—É—î—Ç—å—Å—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ (–≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó?)
2. Answer Relevancy (0-1)     - —á–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑–∞–ø–∏—Ç—É
3. Context Precision (0-1)    - —á–∏ —Ç–æ—á–Ω–∏–π retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç
4. Context Recall (0-1)       - —á–∏ –ø–æ–≤–Ω–∏–π retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç

Production targets:
- Faithfulness: > 0.90 (–∫—Ä–∏—Ç–∏—á–Ω–æ! –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó = –ø—Ä–æ–≤–∞–ª)
- Answer Relevancy: > 0.85
- Context Precision: > 0.80
- Context Recall: > 0.80
"""

import sys
from pathlib import Path
import json
import time
import numpy as np
from typing import Dict, List

# RAGAS imports
try:
    from datasets import Dataset
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    )
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    HAS_RAGAS = True
except ImportError:
    HAS_RAGAS = False
    print("‚ùå RAGAS –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    print("   –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: pip install ragas datasets langchain-openai")
    sys.exit(1)


def load_synthetic_testset(file_path: str = "data/synthetic_testset.json") -> Dict:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏–π —Ç–µ—Å—Ç–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç

    Args:
        file_path: –®–ª—è—Ö –¥–æ JSON —Ñ–∞–π–ª—É –∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–º

    Returns:
        Dict –∑ —Ç–µ—Å—Ç–æ–≤–∏–º–∏ –∫–µ–π—Å–∞–º–∏
    """
    print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É: {file_path}")

    path = Path(file_path)
    if not path.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {file_path}")
        print(f"   –°–ø–æ—á–∞—Ç–∫—É –∑–≥–µ–Ω–µ—Ä—É–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç:")
        print(f"   python generate_synthetic_testset.py")
        sys.exit(1)

    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    num_tests = len(data.get("testcases", []))
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {num_tests} —Ç–µ—Å—Ç–æ–≤–∏—Ö –∫–µ–π—Å—ñ–≤")

    return data


def run_rag_on_testset(rag_system, testset: Dict) -> List[Dict]:
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç–∏ RAG —Å–∏—Å—Ç–µ–º—É –Ω–∞ –≤—Å—ñ—Ö —Ç–µ—Å—Ç–æ–≤–∏—Ö –∫–µ–π—Å–∞—Ö

    Args:
        rag_system: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ RAG —Å–∏—Å—Ç–µ–º–∞ (–∑ –º–µ—Ç–æ–¥–æ–º query())
        testset: –¢–µ—Å—Ç–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏
    """
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ RAG —Å–∏—Å—Ç–µ–º–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ...")

    results = []
    testcases = testset.get("testcases", [])

    print(f"   –û–±—Ä–æ–±–∫–∞ {len(testcases)} –∑–∞–ø–∏—Ç—ñ–≤...")
    print(f"   –¶–µ –∑–∞–π–º–µ ~{len(testcases) * 3 / 60:.1f} —Ö–≤–∏–ª–∏–Ω")
    print()

    start_time = time.time()

    for idx, testcase in enumerate(testcases, 1):
        question = testcase["question"]

        # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å –∫–æ–∂–Ω—ñ 10 –∑–∞–ø–∏—Ç—ñ–≤
        if idx % 10 == 0:
            elapsed = time.time() - start_time
            avg_time = elapsed / idx
            remaining = (len(testcases) - idx) * avg_time
            print(f"   –ü—Ä–æ–≥—Ä–µ—Å: {idx}/{len(testcases)} ({idx/len(testcases)*100:.1f}%) - "
                  f"–ó–∞–ª–∏—à–∏–ª–æ—Å—å ~{remaining/60:.1f} —Ö–≤")

        # –ó–∞–ø—É—Å–∫–∞—î–º–æ RAG
        try:
            result = rag_system.query(question)

            results.append({
                "question": question,
                "answer": result.get("answer", ""),
                "contexts": result.get("contexts", []),
                "ground_truth": testcase.get("ground_truth", ""),
                "evolution_type": testcase.get("evolution_type", "unknown")
            })

        except Exception as e:
            print(f"   ‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –Ω–∞ –∑–∞–ø–∏—Ç—ñ {idx}: {e}")
            # –î–æ–¥–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å
            results.append({
                "question": question,
                "answer": "",
                "contexts": [],
                "ground_truth": testcase.get("ground_truth", ""),
                "evolution_type": testcase.get("evolution_type", "unknown")
            })

    elapsed = time.time() - start_time
    print(f"\n‚úÖ –û–±—Ä–æ–±–ª–µ–Ω–æ: {len(results)} –∑–∞–ø–∏—Ç—ñ–≤ –∑–∞ {elapsed/60:.1f} —Ö–≤")
    print(f"   –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –Ω–∞ –∑–∞–ø–∏—Ç: {elapsed/len(results):.2f}—Å")

    return results


def evaluate_with_ragas(results: List[Dict]) -> Dict:
    """
    –û—Ü—ñ–Ω–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ RAG —Å–∏—Å—Ç–µ–º–∏ —á–µ—Ä–µ–∑ RAGAS

    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏ RAG

    Returns:
        Dict –∑ RAGAS –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    print(f"\nüìä RAGAS Evaluation...")
    print(f"   –ú–µ—Ç—Ä–∏–∫–∏: Faithfulness, Answer Relevancy, Context Precision, Context Recall")
    print(f"   –¶–µ –∑–∞–π–º–µ 2-5 —Ö–≤–∏–ª–∏–Ω –¥–ª—è 50 –∑–∞–ø–∏—Ç—ñ–≤")
    print()

    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è RAGAS
    data = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": []
    }

    for result in results:
        # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        if not result["answer"] or not result["contexts"]:
            continue

        data["question"].append(result["question"])
        data["answer"].append(result["answer"])
        data["contexts"].append(result["contexts"])
        data["ground_truth"].append(result["ground_truth"])

    dataset = Dataset.from_dict(data)

    print(f"‚úÖ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(dataset)} –∑–∞–ø–∏—Ç—ñ–≤ –¥–ª—è evaluation")

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è LLM –¥–ª—è RAGAS
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # –í—Å—ñ 4 –æ—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
    metrics = [
        faithfulness,        # –ß–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ?
        answer_relevancy,    # –ß–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –∑–∞–ø–∏—Ç—É?
        context_precision,   # –ß–∏ —Ç–æ—á–Ω–∏–π retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç?
        context_recall       # –ß–∏ –ø–æ–≤–Ω–∏–π retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç?
    ]

    # –ó–∞–ø—É—Å–∫ evaluation
    start_time = time.time()

    evaluation_result = evaluate(
        dataset,
        metrics=metrics,
        llm=llm,
        embeddings=embeddings
    )

    elapsed = time.time() - start_time

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    def _convert_metric(val):
        if isinstance(val, list):
            return float(np.mean(val))
        return float(val)

    scores = {
        "faithfulness": _convert_metric(evaluation_result["faithfulness"]),
        "answer_relevancy": _convert_metric(evaluation_result["answer_relevancy"]),
        "context_precision": _convert_metric(evaluation_result["context_precision"]),
        "context_recall": _convert_metric(evaluation_result["context_recall"]),
        "queries_evaluated": len(dataset),
        "evaluation_time": elapsed
    }

    # –°–µ—Ä–µ–¥–Ω—ñ–π score
    scores["average_score"] = np.mean([
        scores["faithfulness"],
        scores["answer_relevancy"],
        scores["context_precision"],
        scores["context_recall"]
    ])

    print(f"\n‚úÖ Evaluation –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {elapsed/60:.1f} —Ö–≤")
    print()
    print(f"{'='*70}")
    print(f"üìä RAGAS –ú–ï–¢–†–ò–ö–ò")
    print(f"{'='*70}")
    print(f"Faithfulness:        {scores['faithfulness']:.3f}  {'‚úÖ' if scores['faithfulness'] > 0.90 else '‚ùå'} (target > 0.90)")
    print(f"Answer Relevancy:    {scores['answer_relevancy']:.3f}  {'‚úÖ' if scores['answer_relevancy'] > 0.85 else '‚ö†Ô∏è '} (target > 0.85)")
    print(f"Context Precision:   {scores['context_precision']:.3f}  {'‚úÖ' if scores['context_precision'] > 0.80 else '‚ö†Ô∏è '} (target > 0.80)")
    print(f"Context Recall:      {scores['context_recall']:.3f}  {'‚úÖ' if scores['context_recall'] > 0.80 else '‚ö†Ô∏è '} (target > 0.80)")
    print(f"{'-'*70}")
    print(f"Average Score:       {scores['average_score']:.3f}")
    print(f"{'='*70}")

    return scores


def analyze_by_query_type(results: List[Dict]) -> Dict:
    """
    –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ —Ç–∏–ø–∞—Ö –∑–∞–ø–∏—Ç—ñ–≤

    Args:
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ RAG

    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞—Ö
    """
    print(f"\nüìà –ê–Ω–∞–ª—ñ–∑ –ø–æ —Ç–∏–ø–∞—Ö –∑–∞–ø–∏—Ç—ñ–≤:")

    type_stats = {}

    for result in results:
        evo_type = result.get("evolution_type", "unknown")

        if evo_type not in type_stats:
            type_stats[evo_type] = {
                "count": 0,
                "total_time": 0
            }

        type_stats[evo_type]["count"] += 1

    for evo_type, stats in type_stats.items():
        print(f"   {evo_type}: {stats['count']} –∑–∞–ø–∏—Ç—ñ–≤ ({stats['count']/len(results)*100:.1f}%)")

    return type_stats


def save_evaluation_results(
    scores: Dict,
    results: List[Dict],
    output_file: str = "results/ragas_evaluation_full.json"
):
    """
    –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ evaluation

    Args:
        scores: RAGAS –º–µ—Ç—Ä–∏–∫–∏
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ RAG
        output_file: –®–ª—è—Ö –¥–æ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    """
    print(f"\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤: {output_file}")

    output = {
        "metadata": {
            "evaluated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_queries": len(results),
            "ragas_version": "latest"
        },
        "ragas_scores": scores,
        "detailed_results": results[:10]  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ª–∏—à–µ –ø–µ—Ä—à—ñ 10 –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É
    }

    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ!")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è - evaluation RAG –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ"""
    print("="*80)
    print("üìä RAG EVALUATION –ó –°–ò–ù–¢–ï–¢–ò–ß–ù–ò–ú –î–ê–¢–ê–°–ï–¢–û–ú (RAGAS)")
    print("="*80)
    print()

    if not HAS_RAGAS:
        return

    # –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
    testset = load_synthetic_testset("data/synthetic_testset.json")

    # –ö—Ä–æ–∫ 2: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è RAG —Å–∏—Å—Ç–µ–º–∏
    print(f"\n‚öôÔ∏è  –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è RAG —Å–∏—Å—Ç–µ–º–∏...")
    print(f"   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ: Advanced RAG (–Ω–∞–π–∫—Ä–∞—â–∏–π –±–∞–ª–∞–Ω—Å)")

    sys.path.append(str(Path(__file__).parent))
    from advanced_rag.advanced_rag_demo import AdvancedRAG

    rag = AdvancedRAG(
        documents_path="data/pdfs",
        chunk_size=500,
        chunk_overlap=100
    )

    print(f"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤...")
    rag.load_and_process_documents(max_documents=None)  # –í—Å—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏!

    print(f"   –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤...")
    rag.create_embeddings()

    print(f"‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞!")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {len(rag.chunks)} —á–∞–Ω–∫—ñ–≤")

    # –ö—Ä–æ–∫ 3: –ó–∞–ø—É—Å–∫ RAG –Ω–∞ –≤—Å—ñ—Ö —Ç–µ—Å—Ç–∞—Ö
    results = run_rag_on_testset(rag, testset)

    # –ö—Ä–æ–∫ 4: RAGAS Evaluation
    scores = evaluate_with_ragas(results)

    # –ö—Ä–æ–∫ 5: –ê–Ω–∞–ª—ñ–∑ –ø–æ —Ç–∏–ø–∞—Ö
    type_stats = analyze_by_query_type(results)

    # –ö—Ä–æ–∫ 6: –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    save_evaluation_results(scores, results)

    # –í–∏—Å–Ω–æ–≤–∫–∏
    print(f"\n{'='*80}")
    print(f"‚úÖ EVALUATION –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"{'='*80}")
    print()

    if scores["average_score"] >= 0.85:
        print("üéâ –í–Ü–î–ú–Ü–ù–ù–û! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è production")
    elif scores["average_score"] >= 0.75:
        print("‚ö†Ô∏è  –ü–†–ò–ô–ù–Ø–¢–ù–û, –∞–ª–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è")
    else:
        print("‚ùå –ù–ò–ó–¨–ö–ê –Ø–ö–Ü–°–¢–¨ - –ø–æ—Ç—Ä—ñ–±–Ω–∞ —Å–µ—Ä–π–æ–∑–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è")

    print()
    print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:")
    if scores["faithfulness"] < 0.90:
        print("  - Faithfulness –Ω–∏–∑—å–∫–∏–π ‚Üí –¥–æ–¥–∞–π—Ç–µ re-ranking –∞–±–æ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É")
    if scores["context_recall"] < 0.80:
        print("  - Context Recall –Ω–∏–∑—å–∫–∏–π ‚Üí –∑–±—ñ–ª—å—à—Ç–µ top_k –∞–±–æ –ø–æ–∫—Ä–∞—â—ñ—Ç—å retrieval")
    if scores["context_precision"] < 0.80:
        print("  - Context Precision –Ω–∏–∑—å–∫–∏–π ‚Üí –ø–æ–∫—Ä–∞—â—ñ—Ç—å —è–∫—ñ—Å—Ç—å embeddings")
    if scores["answer_relevancy"] < 0.85:
        print("  - Answer Relevancy –Ω–∏–∑—å–∫–∏–π ‚Üí –æ–ø—Ç–∏–º—ñ–∑—É–π—Ç–µ LLM prompt")

    print()


if __name__ == "__main__":
    main()
