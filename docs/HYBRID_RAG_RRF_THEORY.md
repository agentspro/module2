# üîÄ Hybrid RAG –∑ Reciprocal Rank Fusion (RRF) - –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∏–π –ì–∞–π–¥

**–î–ª—è**: RAG Workshop - –ú–æ–¥—É–ª—å 2
**–°—Ç–∞—Ç—É—Å**: –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è (implementation has bug)
**Use Case**: –ë–∞–ª–∞–Ω—Å—É–≤–∞—Ç–∏ keyword search (BM25) —Ç–∞ semantic search (embeddings)

---

## üéØ –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è Hybrid RAG

Hybrid RAG –∫–æ–º–±—ñ–Ω—É—î **–¥–≤–∞ —Ä—ñ–∑–Ω—ñ –º–µ—Ç–æ–¥–∏ –ø–æ—à—É–∫—É**:

1. **Sparse Retrieval** (BM25/TF-IDF) - keyword-based, —à–≤–∏–¥–∫–∏–π, —Ç–æ—á–Ω–∏–π –¥–ª—è exact matches
2. **Dense Retrieval** (FAISS/embeddings) - semantic, —Ä–æ–∑—É–º—ñ—î meaning, –ø—Ä–∞—Ü—é—î –∑ synonyms

### –ù–∞–≤—ñ—â–æ –ö–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏?

```
Query: "Python machine learning frameworks"

Sparse (BM25):
‚úÖ –ó–Ω–∞–π–¥–µ: "Python", "machine learning", "frameworks" (exact words)
‚ùå –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å: "PyTorch deep neural network library" (–Ω–µ–º–∞ —Å–ª–æ–≤–∞ "framework")

Dense (Embeddings):
‚úÖ –ó–Ω–∞–π–¥–µ: "PyTorch deep neural network library" (—Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ —Å—Ö–æ–∂–µ)
‚ùå –ú–æ–∂–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏: –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∑ exact match –∞–ª–µ low semantic similarity

Hybrid (RRF):
‚úÖ –û–±'—î–¥–Ω—É—î –æ–±–∏–¥–≤–∞! –ó–Ω–∞–π–¥–µ —ñ exact matches, —ñ semantic matches
```

---

## üî¢ Reciprocal Rank Fusion (RRF) - –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π –ê–ª–≥–æ—Ä–∏—Ç–º

### –§–æ—Ä–º—É–ª–∞ RRF

```
RRF_score(doc) = (1 - Œ±) √ó (1 / (k + rank_sparse)) + Œ± √ó (1 / (k + rank_dense))

–¥–µ:
- rank_sparse: –ø–æ–∑–∏—Ü—ñ—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ sparse results (1, 2, 3, ...)
- rank_dense: –ø–æ–∑–∏—Ü—ñ—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ dense results (1, 2, 3, ...)
- k: –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ (–∑–∞–∑–≤–∏—á–∞–π 60) - –∑–º–µ–Ω—à—É—î –≤–∞–≥—É —Ä—ñ–∑–Ω–∏—Ü—ñ –º—ñ–∂ —Ç–æ–ø –ø–æ–∑–∏—Ü—ñ—è–º–∏
- Œ±: weight parameter (0 to 1) - –±–∞–ª–∞–Ω—Å –º—ñ–∂ sparse —Ç–∞ dense
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

#### k (rank constant)
```
k = 60 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó RRF paper)

–ü—Ä–∏–∫–ª–∞–¥ –≤–ø–ª–∏–≤—É:
rank = 1:  1/(60+1) = 0.0164
rank = 2:  1/(60+2) = 0.0161
rank = 10: 1/(60+10) = 0.0143

üëâ –ú–∞–ª—ñ –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –º—ñ–∂ top results - –¥–æ–±—Ä–µ –¥–ª—è robustness
```

#### Œ± (fusion weight)
```
Œ± = 0.3  ‚Üí 70% sparse, 30% dense  (favor keywords)
Œ± = 0.5  ‚Üí 50% sparse, 50% dense  (balanced)
Œ± = 0.7  ‚Üí 30% sparse, 70% dense  (favor semantic)

–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
- Technical docs (API, code):     Œ± = 0.3 (exact terms –≤–∞–∂–ª–∏–≤—ñ—à—ñ)
- General knowledge:               Œ± = 0.5 (balanced)
- Natural language queries:        Œ± = 0.7 (semantic –≤–∞–∂–ª–∏–≤—ñ—à–∏–π)
- Corporate docs (benchmarked):    Œ± = 0.65 (optimal)
```

---

## üêõ –ë–∞–≥ –≤ –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó

### –ü—Ä–æ–±–ª–µ–º–∞

**–°–∏–º–ø—Ç–æ–º**: –í—Å—ñ RRF scores = 0.008 (–æ–¥–Ω–∞–∫–æ–≤—ñ –¥–ª—è –≤—Å—ñ—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤)

### –ú–æ–∂–ª–∏–≤—ñ –ü—Ä–∏—á–∏–Ω–∏:

#### 1. **–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞ –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è**

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û:
def reciprocal_rank_fusion(sparse_results, dense_results, alpha=0.5):
    scores = {}
    for doc in all_docs:
        # –ü—Ä–æ–±–ª–µ–º–∞: rank –Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        sparse_rank = sparse_results.index(doc) if doc in sparse_results else 9999
        dense_rank = dense_results.index(doc) if doc in dense_results else 9999

        # –ü—Ä–æ–±–ª–µ–º–∞: –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –¥–æ fusion
        score = (1-alpha) * (sparse_rank / len(sparse_results)) + \
                alpha * (dense_rank / len(dense_results))
        scores[doc] = score
    return scores

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –≤—Å—ñ scores –±–ª–∏–∑—å–∫—ñ –¥–æ 0.5, –Ω–µ–º–∞—î –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç–µ–π
```

#### 2. **–í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å k –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏**

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û:
score = 1 / rank  # –ë–µ–∑ k –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏

# –ü—Ä–æ–±–ª–µ–º–∞:
# rank=1: 1/1 = 1.0
# rank=2: 1/2 = 0.5  ‚Üê –ó–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∞ —Ä—ñ–∑–Ω–∏—Ü—è!
# rank=10: 1/10 = 0.1

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û:
score = 1 / (k + rank)  # –ó k=60

# rank=1: 1/61 = 0.0164
# rank=2: 1/62 = 0.0161  ‚Üê –ú–µ–Ω—à–∞ —Ä—ñ–∑–Ω–∏—Ü—è (–∫—Ä–∞—â–µ!)
# rank=10: 1/70 = 0.0143
```

#### 3. **–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π Ranking (0-indexed vs 1-indexed)**

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û:
for i, doc in enumerate(results):
    rank = i  # –ü–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ 0!

# rank=0 ‚Üí 1/(60+0) = 0.0167
# rank=1 ‚Üí 1/(60+1) = 0.0164

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û:
for i, doc in enumerate(results):
    rank = i + 1  # –ü–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ 1!

# rank=1 ‚Üí 1/(60+1) = 0.0164
# rank=2 ‚Üí 1/(60+2) = 0.0161
```

---

## ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞ –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è RRF

### –ü—Å–µ–≤–¥–æ–∫–æ–¥

```python
def reciprocal_rank_fusion(sparse_results, dense_results, alpha=0.5, k=60):
    """
    –ü—Ä–∞–≤–∏–ª—å–Ω–∞ RRF fusion

    Parameters:
    - sparse_results: List[(doc_id, score)] –≤—ñ–¥ BM25/TF-IDF
    - dense_results: List[(doc_id, score)] –≤—ñ–¥ FAISS/embeddings
    - alpha: weight (0=only sparse, 1=only dense)
    - k: rank constant (default 60)

    Returns:
    - List[(doc_id, rrf_score)] sorted by RRF score
    """

    # –ö—Ä–æ–∫ 1: –°—Ç–≤–æ—Ä–∏—Ç–∏ rank dictionaries
    sparse_ranks = {}
    dense_ranks = {}

    for rank, (doc_id, _) in enumerate(sparse_results, start=1):
        sparse_ranks[doc_id] = rank

    for rank, (doc_id, _) in enumerate(dense_results, start=1):
        dense_ranks[doc_id] = rank

    # –ö—Ä–æ–∫ 2: –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏
    all_docs = set(sparse_ranks.keys()) | set(dense_ranks.keys())

    # –ö—Ä–æ–∫ 3: –û–±—á–∏—Å–ª–∏—Ç–∏ RRF scores
    rrf_scores = {}

    for doc_id in all_docs:
        # –û—Ç—Ä–∏–º–∞—Ç–∏ ranks (—è–∫—â–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ–º–∞—î, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –¥—É–∂–µ –≤–µ–ª–∏–∫–∏–π rank)
        sparse_rank = sparse_ranks.get(doc_id, len(sparse_results) + 100)
        dense_rank = dense_ranks.get(doc_id, len(dense_results) + 100)

        # RRF formula
        sparse_score = 1.0 / (k + sparse_rank)
        dense_score = 1.0 / (k + dense_rank)

        # Weighted fusion
        rrf_score = (1 - alpha) * sparse_score + alpha * dense_score

        rrf_scores[doc_id] = rrf_score

    # –ö—Ä–æ–∫ 4: –°–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ RRF score (descending)
    sorted_results = sorted(
        rrf_scores.items(),
        key=lambda x: x[1],
        reverse=True
    )

    return sorted_results
```

### –ü—Ä–∏–∫–ª–∞–¥ –†–æ–±–æ—Ç–∏

```python
# –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ
sparse_results = [
    ("doc1", 0.95),  # rank=1
    ("doc2", 0.80),  # rank=2
    ("doc3", 0.60),  # rank=3
]

dense_results = [
    ("doc2", 0.99),  # rank=1
    ("doc4", 0.85),  # rank=2
    ("doc1", 0.70),  # rank=3
]

# RRF calculation (Œ±=0.5, k=60)
# doc1:
#   sparse: 1/(60+1) = 0.0164
#   dense:  1/(60+3) = 0.0159
#   RRF: 0.5 * 0.0164 + 0.5 * 0.0159 = 0.0161

# doc2:
#   sparse: 1/(60+2) = 0.0161
#   dense:  1/(60+1) = 0.0164
#   RRF: 0.5 * 0.0161 + 0.5 * 0.0164 = 0.0163  ‚Üê –ù–∞–π–≤–∏—â–∏–π!

# doc3:
#   sparse: 1/(60+3) = 0.0159
#   dense:  1/(60+103) = 0.0061  (–Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Üí rank=100+3)
#   RRF: 0.5 * 0.0159 + 0.5 * 0.0061 = 0.0110

# doc4:
#   sparse: 1/(60+103) = 0.0061  (–Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ)
#   dense:  1/(60+2) = 0.0161
#   RRF: 0.5 * 0.0061 + 0.5 * 0.0161 = 0.0111

# –§—ñ–Ω–∞–ª—å–Ω–∏–π ranking:
# 1. doc2 (0.0163) - —Ç–æ–ø –≤ –æ–±–æ—Ö!
# 2. doc1 (0.0161) - –¥–æ–±—Ä–∏–π –≤ –æ–±–æ—Ö
# 3. doc4 (0.0111) - —Ç—ñ–ª—å–∫–∏ –≤ dense
# 4. doc3 (0.0110) - —Ç—ñ–ª—å–∫–∏ –≤ sparse
```

---

## üìä –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –Ü–Ω—à–∏–º–∏ –ú–µ—Ç–æ–¥–∞–º–∏

### 1. Linear Combination (Baseline)

```python
score = Œ± √ó sparse_score + (1-Œ±) √ó dense_score

–ü—Ä–æ–±–ª–µ–º–∞:
- –ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∞–±—Å–æ–ª—é—Ç–Ω–∏—Ö scores
- –†—ñ–∑–Ω—ñ scale (BM25: 0-‚àû, cosine similarity: 0-1)
- –ü–æ—Ç—Ä–µ–±—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
```

### 2. Max Score

```python
score = max(sparse_score, dense_score)

–ü—Ä–æ–±–ª–µ–º–∞:
- –Ü–≥–Ω–æ—Ä—É—î consensus (–∫–æ–ª–∏ –æ–±–∏–¥–≤–∞ –º–µ—Ç–æ–¥–∏ agree)
- –ó–∞–Ω–∞–¥—Ç–æ –∞–≥—Ä–µ—Å–∏–≤–Ω–µ
```

### 3. RRF (–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)

```
‚úÖ –ù–µ –ø–æ—Ç—Ä–µ–±—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó scores
‚úÖ –ü—Ä–∞—Ü—é—î –∑ rankings (position-based)
‚úÖ Robust –¥–æ outliers
‚úÖ –ü—Ä–æ—Å—Ç–∏–π —É implementation
‚úÖ –î–æ–≤–µ–¥–µ–Ω–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å (TREC competitions)
```

---

## üéØ –í–∏–±—ñ—Ä Alpha Parameter

### –ú–µ—Ç–æ–¥–æ–ª–æ–≥—ñ—è

```python
# –ö—Ä–æ–∫ 1: –°—Ç–≤–æ—Ä–∏—Ç–∏ test queries –∑ ground truth
test_queries = [
    {
        "query": "Python machine learning",
        "relevant_docs": ["doc1", "doc3", "doc7"]
    },
    # ... 20-50 queries
]

# –ö—Ä–æ–∫ 2: –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ —Ä—ñ–∑–Ω—ñ Œ±
alphas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

results = {}
for alpha in alphas:
    scores = []
    for test_q in test_queries:
        rrf_results = reciprocal_rank_fusion(..., alpha=alpha)
        # Calculate precision/recall/F1
        score = evaluate_results(rrf_results, test_q["relevant_docs"])
        scores.append(score)

    results[alpha] = np.mean(scores)

# –ö—Ä–æ–∫ 3: –í–∏–±—Ä–∞—Ç–∏ optimal Œ±
best_alpha = max(results, key=results.get)
```

### –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ (Corporate Docs Benchmark)

```
Œ± = 0.0  (only sparse):  F1 = 0.42
Œ± = 0.3:                 F1 = 0.68
Œ± = 0.5:                 F1 = 0.72
Œ± = 0.65:                F1 = 0.78  ‚Üê Optimal!
Œ± = 0.7:                 F1 = 0.76
Œ± = 1.0  (only dense):   F1 = 0.58
```

**–í–∏—Å–Ω–æ–≤–æ–∫**: Œ±=0.65 (65% semantic, 35% keyword) –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.

---

## üí° Use Cases

### –ö–æ–ª–∏ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Hybrid RAG?

#### ‚úÖ –î–æ–±—Ä—ñ Use Cases:

1. **Technical Documentation**
   - –ü–æ—Ç—Ä—ñ–±–Ω—ñ exact terms (API names, commands)
   - –ê–ª–µ —Ç–∞–∫–æ–∂ semantic understanding
   - **Œ± = 0.3-0.4** (favor keywords)

2. **Legal/Medical Documents**
   - Exact terminology –∫—Ä–∏—Ç–∏—á–Ω–∞
   - –ê–ª–µ synonyms —Ç–µ–∂ –≤–∞–∂–ª–∏–≤—ñ
   - **Œ± = 0.4-0.5** (balanced)

3. **General Knowledge Base**
   - Mix of technical —Ç–∞ natural language
   - **Œ± = 0.5-0.6** (balanced to semantic)

4. **Customer Support**
   - Users use different terms
   - Semantic similarity –≤–∞–∂–ª–∏–≤–∞
   - **Œ± = 0.6-0.7** (favor semantic)

#### ‚ùå –ù–µ –†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è:

1. **Pure code search** ‚Üí Use only sparse (BM25)
2. **Conceptual similarity** ‚Üí Use only dense (embeddings)
3. **Real-time systems** ‚Üí RRF –¥–æ–¥–∞—î latency (–ø–æ—Ç—Ä—ñ–±–Ω—ñ 2 retrievals)

---

## ‚ö° Performance Considerations

### Latency

```
Naive approach (sequential):
1. Sparse retrieval:  50ms
2. Dense retrieval:   200ms
3. RRF fusion:        5ms
Total:                255ms

Optimized (parallel):
1. Sparse + Dense:    200ms (parallel)
2. RRF fusion:        5ms
Total:                205ms  ‚Üê 20% —à–≤–∏–¥—à–µ!
```

### –ö–æ–¥ –¥–ª—è Parallel Execution

```python
import concurrent.futures

def hybrid_search_parallel(query, top_k=10):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        # Launch both retrievals in parallel
        sparse_future = executor.submit(bm25_search, query, top_k=20)
        dense_future = executor.submit(faiss_search, query, top_k=20)

        # Wait for both
        sparse_results = sparse_future.result()
        dense_results = dense_future.result()

    # RRF fusion
    final_results = reciprocal_rank_fusion(
        sparse_results,
        dense_results,
        alpha=0.65
    )

    return final_results[:top_k]
```

---

## üîß Production Checklist

### –ü–µ—Ä–µ–¥ Deployment:

- [ ] **Tune Œ±** parameter –Ω–∞ –≤–∞—à–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ default 0.5!)
- [ ] **Implement parallel retrieval** (sparse + dense –æ–¥–Ω–æ—á–∞—Å–Ω–æ)
- [ ] **Add caching** –¥–ª—è frequently queried –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
- [ ] **Monitor rankings** (—á–∏ RRF –¥–∞—î –∫—Ä–∞—â—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏?)
- [ ] **A/B test** –ø—Ä–æ—Ç–∏ pure sparse –∞–±–æ pure dense
- [ ] **Set k=60** (–Ω–µ –∑–º—ñ–Ω—é–π—Ç–µ –±–µ–∑ –≤–µ—Å–∫–æ–π –ø—Ä–∏—á–∏–Ω–∏)
- [ ] **Handle edge cases**:
  - –ü—É—Å—Ç—ñ results –≤—ñ–¥ sparse/dense
  - –û–¥–Ω–∞–∫–æ–≤—ñ RRF scores (tie-breaking)
  - –î—É–∂–µ –≤–µ–ª–∏–∫—ñ result sets (>1000 docs)

---

## üìö –î–æ–¥–∞—Ç–∫–æ–≤—ñ –†–µ—Å—É—Ä—Å–∏

### Papers

1. **Original RRF Paper**:
   - "Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods"
   - Cormack, Clarke, B√ºttcher (2009)
   - SIGIR Conference

2. **Hybrid Search in Production**:
   - "Hybrid Retrieval for Open-Domain Question Answering"
   - Karpukhin et al. (2020)
   - Facebook AI Research

### Libraries

```python
# Rank-BM25 (sparse)
pip install rank-bm25

# FAISS (dense)
pip install faiss-cpu  # or faiss-gpu

# Example usage
from rank_bm25 import BM25Okapi
import faiss

# Sparse
bm25 = BM25Okapi(tokenized_corpus)
sparse_scores = bm25.get_scores(tokenized_query)

# Dense
index = faiss.IndexFlatIP(embedding_dim)
index.add(doc_embeddings)
dense_scores, doc_ids = index.search(query_embedding, k=20)

# RRF
results = reciprocal_rank_fusion(sparse_results, dense_results)
```

---

## üéì –î–ª—è –í–æ—Ä–∫—à–æ–ø—É

### –ö–ª—é—á–æ–≤—ñ –ú–µ—Å–µ–¥–∂—ñ (3 —Ö–≤):

1. **–ü—Ä–æ–±–ª–µ–º–∞**: Sparse –∑–Ω–∞—î exact words, Dense –∑–Ω–∞—î meaning
2. **–†—ñ—à–µ–Ω–Ω—è**: RRF –∫–æ–º–±—ñ–Ω—É—î rankings –∑ –æ–±–æ—Ö –º–µ—Ç–æ–¥—ñ–≤
3. **–§–æ—Ä–º—É–ª–∞**: `1/(k+rank)` –∑ weighted combination
4. **–ü–∞—Ä–∞–º–µ—Ç—Ä Œ±**: 0.65 optimal –¥–ª—è corporate docs (benchmark)
5. **Parallel**: –ó–∞–ø—É—Å–∫–∞–π—Ç–µ sparse + dense –æ–¥–Ω–æ—á–∞—Å–Ω–æ

### –î–µ–º–æ (–Ω–µ –∑–∞–ø—É—Å–∫–∞—Ç–∏, show slides):

```
Query: "How to train neural networks?"

Sparse (BM25) finds:
- "Neural network training guide"       ‚Üê exact match
- "Train AI models step-by-step"        ‚Üê has "train"

Dense (FAISS) finds:
- "Deep learning model optimization"    ‚Üê semantic match
- "Backpropagation algorithms"          ‚Üê related concept

RRF combines:
1. "Neural network training guide"      ‚Üê top in sparse, good in dense
2. "Deep learning model optimization"   ‚Üê top in dense, ok in sparse
3. "Train AI models step-by-step"       ‚Üê good in sparse
4. "Backpropagation algorithms"         ‚Üê good in dense

üëâ Balanced results, leveraging both methods!
```

---

**–°—Ç–∞—Ç—É—Å**: –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –≥–æ—Ç–æ–≤–∏–π –¥–ª—è –≤–æ—Ä–∫—à–æ–ø—É ‚úÖ
**–ß–∞—Å –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—ó**: 3-5 —Ö–≤–∏–ª–∏–Ω (—É –±–ª–æ—Ü—ñ Hybrid RAG)
**–ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏**: –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –Ω–∞ –≤–æ—Ä–∫—à–æ–ø—ñ –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω–Ω—è –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó (–±–µ–∑ –∑–∞–ø—É—Å–∫—É –∫–æ–¥—É)

---

**–°—Ç–≤–æ—Ä–µ–Ω–æ**: 25 –∂–æ–≤—Ç–Ω—è 2025
**–î–ª—è**: RAG Workshop - –ú–æ–¥—É–ª—å 2
**–í–µ—Ä—Å—ñ—è**: 1.0 - –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∏–π –≥–∞–π–¥

**–ü—Ä–∏–º—ñ—Ç–∫–∞**: –§–∞–∫—Ç–∏—á–Ω–∞ implementation Hybrid RAG –∑ RRF –º–∞—î –±–∞–≥ —ñ –Ω–µ –≥–æ—Ç–æ–≤–∞ –¥–æ production. –¶–µ–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ—è—Å–Ω—é—î —è–∫ *–º–∞—î* –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è.
