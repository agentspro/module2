"""
–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è RAG —á–µ—Ä–µ–∑ RAGAS
==============================================================

RAGAS –º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ —Ç–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∏ –∑ PDF –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤:
- Simple queries (–ø—Ä–æ—Å—Ç—ñ —Ñ–∞–∫—Ç–∏)
- Reasoning queries (–ø–æ—Ç—Ä–µ–±—É—é—Ç—å —Ä–æ–∑–¥—É–º—ñ–≤)
- Multi-context queries (–ø–æ—Ç—Ä–µ–±—É—é—Ç—å –∫—ñ–ª—å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤)
- Conditional queries (—Å–∫–ª–∞–¥–Ω—ñ —É–º–æ–≤–Ω—ñ –∑–∞–ø–∏—Ç–∏)

–†–µ–∑—É–ª—å—Ç–∞—Ç: 50-200+ —è–∫—ñ—Å–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤–∏—Ö –∫–µ–π—Å—ñ–≤ –∑–∞–º—ñ—Å—Ç—å 6 —Ö–∞—Ä–¥–∫–æ–¥–∂–µ–Ω–∏—Ö!
"""

import sys
import os
from pathlib import Path
import json
import time
from dotenv import load_dotenv

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –∑ .env (—à—É–∫–∞—î–º–æ –≤ –ø–æ—Ç–æ—á–Ω—ñ–π —Ç–∞ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó)
load_dotenv()  # –°–ø–æ—á–∞—Ç–∫—É –ø–æ—Ç–æ—á–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è
if not os.getenv('OPENAI_API_KEY'):
    # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏, —à—É–∫–∞—î–º–æ –≤ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    load_dotenv(Path(__file__).parent.parent / '.env')

# RAGAS imports –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
try:
    from ragas.testset.generator import TestsetGenerator
    from ragas.testset.evolutions import simple, reasoning, multi_context
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
    HAS_RAGAS = True
except ImportError:
    HAS_RAGAS = False
    print("‚ùå RAGAS –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    print("   –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: pip install ragas langchain-openai langchain-community pypdf")
    sys.exit(1)


def load_documents_for_generation(pdf_dir: str = "data/pdfs"):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤

    Args:
        pdf_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –∑ PDF —Ñ–∞–π–ª–∞–º–∏

    Returns:
        List of Document objects –¥–ª—è RAGAS
    """
    print(f"\nüìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ {pdf_dir}...")

    loader = DirectoryLoader(
        pdf_dir,
        glob="**/*.pdf",
        loader_cls=PyPDFLoader,
        show_progress=True
    )

    documents = loader.load()
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {len(documents)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –∑ PDF")

    return documents


def generate_testset(
    documents,
    test_size: int = 50,
    distributions: dict = None
):
    """
    –ì–µ–Ω–µ—Ä—É—î —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏–π —Ç–µ—Å—Ç–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç —á–µ—Ä–µ–∑ RAGAS

    Args:
        documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (LangChain Documents)
        test_size: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        distributions: –†–æ–∑–ø–æ–¥—ñ–ª —Ç–∏–ø—ñ–≤ –∑–∞–ø–∏—Ç—ñ–≤

    Returns:
        Generated testset
    """
    print(f"\nüß™ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è {test_size} —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤...")
    print("   –¶–µ –∑–∞–π–º–µ 3-5 —Ö–≤–∏–ª–∏–Ω –¥–ª—è 50 –∑–∞–ø–∏—Ç—ñ–≤")
    print()

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è LLM —Ç–∞ embeddings –¥–ª—è RAGAS
    generator_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    critic_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # –°—Ç–≤–æ—Ä—é—î–º–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    generator = TestsetGenerator.from_langchain(
        generator_llm=generator_llm,
        critic_llm=critic_llm,
        embeddings=embeddings
    )

    # –†–æ–∑–ø–æ–¥—ñ–ª —Ç–∏–ø—ñ–≤ –∑–∞–ø–∏—Ç—ñ–≤ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
    if distributions is None:
        distributions = {
            simple: 0.4,        # 40% - –ø—Ä–æ—Å—Ç—ñ —Ñ–∞–∫—Ç–∏—á–Ω—ñ –∑–∞–ø–∏—Ç–∏
            reasoning: 0.3,     # 30% - –ø–æ—Ç—Ä–µ–±—É—é—Ç—å —Ä–æ–∑–¥—É–º—ñ–≤
            multi_context: 0.3  # 30% - –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –∫—ñ–ª—å–∫–æ—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
        }

    print("üìä –†–æ–∑–ø–æ–¥—ñ–ª —Ç–∏–ø—ñ–≤ –∑–∞–ø–∏—Ç—ñ–≤:")
    print(f"   Simple (—Ñ–∞–∫—Ç–∏):          {distributions.get(simple, 0)*100:.0f}%")
    print(f"   Reasoning (—Ä–æ–∑–¥—É–º–∏):     {distributions.get(reasoning, 0)*100:.0f}%")
    print(f"   Multi-context (—Å–∫–ª–∞–¥–Ω—ñ): {distributions.get(multi_context, 0)*100:.0f}%")
    print()

    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è
    start_time = time.time()

    testset = generator.generate_with_langchain_docs(
        documents,
        test_size=test_size,
        distributions=distributions,
        raise_exceptions=False  # –ù–µ –ø–∞–¥–∞—Ç–∏ –Ω–∞ –ø–æ–º–∏–ª–∫–∞—Ö
    )

    elapsed = time.time() - start_time

    print(f"‚úÖ –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ: {len(testset)} —Ç–µ—Å—Ç–æ–≤–∏—Ö –∫–µ–π—Å—ñ–≤ –∑–∞ {elapsed:.1f}—Å")

    return testset


def save_testset(testset, output_file: str = "data/synthetic_testset.json"):
    """
    –ó–±–µ—Ä–µ–≥—Ç–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç —É JSON

    Args:
        testset: RAGAS testset object
        output_file: –®–ª—è—Ö –¥–æ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    """
    print(f"\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É: {output_file}")

    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ testset —É DataFrame –ø–æ—Ç—ñ–º —É dict
    df = testset.to_pandas()

    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    data = {
        "metadata": {
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_questions": len(df),
            "source": "RAGAS synthetic generation from PDFs"
        },
        "testcases": []
    }

    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –∫–æ–∂–µ–Ω —Ç–µ—Å—Ç–æ–≤–∏–π –∫–µ–π—Å
    for idx, row in df.iterrows():
        testcase = {
            "question": row.get("question", ""),
            "ground_truth": row.get("ground_truth", ""),
            "contexts": row.get("contexts", []),
            "evolution_type": row.get("evolution_type", "unknown"),
            "metadata": row.get("metadata", {})
        }
        data["testcases"].append(testcase)

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {len(data['testcases'])} —Ç–µ—Å—Ç–æ–≤–∏—Ö –∫–µ–π—Å—ñ–≤")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—É:")
    evolution_counts = df['evolution_type'].value_counts()
    for evo_type, count in evolution_counts.items():
        print(f"   {evo_type}: {count} ({count/len(df)*100:.1f}%)")


def preview_testset(testset, num_examples: int = 5):
    """
    –ü–æ–∫–∞–∑–∞—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤–∏—Ö –∫–µ–π—Å—ñ–≤

    Args:
        testset: RAGAS testset
        num_examples: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –ø–æ–∫–∞–∑—É
    """
    print(f"\n{'='*80}")
    print(f"üìù –ü–†–ò–ö–õ–ê–î–ò –ó–ì–ï–ù–ï–†–û–í–ê–ù–ò–• –¢–ï–°–¢–û–í–ò–• –ö–ï–ô–°–Ü–í (–ø–µ—Ä—à—ñ {num_examples})")
    print(f"{'='*80}\n")

    df = testset.to_pandas()

    for idx in range(min(num_examples, len(df))):
        row = df.iloc[idx]

        print(f"–ö–µ–π—Å #{idx+1}")
        print(f"–¢–∏–ø: {row.get('evolution_type', 'unknown')}")
        print(f"–ó–∞–ø–∏—Ç: {row.get('question', '')}")
        print(f"Ground Truth: {row.get('ground_truth', '')[:200]}...")
        print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç–∏: {len(row.get('contexts', []))} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
        print("-" * 80)
        print()


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è - –≥–µ–Ω–µ—Ä—É—î —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç"""
    print("="*80)
    print("üß™ RAGAS: –ì–ï–ù–ï–†–ê–¶–Ü–Ø –°–ò–ù–¢–ï–¢–ò–ß–ù–û–ì–û –¢–ï–°–¢–û–í–û–ì–û –î–ê–¢–ê–°–ï–¢–£")
    print("="*80)
    print()
    print("–¶–µ–π —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–≥–µ–Ω–µ—Ä—É—î —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ —Ç–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∏")
    print("–∑ –≤–∞—à–∏—Ö PDF –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é RAGAS.")
    print()

    if not HAS_RAGAS:
        return

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
    PDF_DIR = "data/pdfs"
    TEST_SIZE = 50  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
    OUTPUT_FILE = "data/synthetic_testset.json"

    print(f"‚öôÔ∏è  –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:")
    print(f"   PDF –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è: {PDF_DIR}")
    print(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–µ—Å—Ç—ñ–≤: {TEST_SIZE}")
    print(f"   –í–∏—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª: {OUTPUT_FILE}")
    print()

    try:
        # –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
        documents = load_documents_for_generation(PDF_DIR)

        if not documents:
            print("‚ùå –î–æ–∫—É–º–µ–Ω—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
            print(f"   –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—å —â–æ PDF —Ñ–∞–π–ª–∏ —î –≤ {PDF_DIR}")
            return

        # –ö—Ä–æ–∫ 2: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
        testset = generate_testset(
            documents,
            test_size=TEST_SIZE
        )

        # –ö—Ä–æ–∫ 3: –ü–æ–∫–∞–∑–∞—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥–∏
        preview_testset(testset, num_examples=3)

        # –ö—Ä–æ–∫ 4: –ó–±–µ—Ä–µ–≥—Ç–∏
        save_testset(testset, OUTPUT_FILE)

        print("\n" + "="*80)
        print("‚úÖ –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("="*80)
        print()
        print("–ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:")
        print(f"1. –ü–µ—Ä–µ–≥–ª—è–Ω—å—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç: {OUTPUT_FILE}")
        print(f"2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ –¥–ª—è evaluation: python evaluate_with_synthetic_testset.py")
        print()

    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
