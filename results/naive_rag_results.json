{
  "system_name": "Naive RAG",
  "total_documents": 50,
  "total_chunks": 9198,
  "chunk_size": 500,
  "chunk_overlap": 100,
  "llm_model": "fallback (без LLM)",
  "queries": [
    {
      "question": "What is Retrieval-Augmented Generation (RAG)?",
      "answer": "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.\n\nry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.33507892326984534,
        0.33156012807156204,
        0.3287575743280185
      ],
      "contexts": [
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
        "ry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?"
      ],
      "execution_time": 0.5166125297546387,
      "category": "definition",
      "query_id": 1,
      "difficulty": "easy"
    },
    {
      "question": "What are the main components of a RAG system?",
      "answer": "to the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\n\nND FUTURE RESEARCH\nDIRECTIONS\nThe lessons learned from the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\n\na human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?",
      "relevant_chunks": 3,
      "sources": [
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf"
      ],
      "scores": [
        0.24336407203790006,
        0.21575490725500665,
        0.2104894025260209
      ],
      "contexts": [
        "to the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using",
        "ND FUTURE RESEARCH\nDIRECTIONS\nThe lessons learned from the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular",
        "a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?"
      ],
      "execution_time": 0.5280113220214844,
      "category": "definition",
      "query_id": 2,
      "difficulty": "easy"
    },
    {
      "question": "How does RAG differ from traditional LLMs?",
      "answer": "e then used to rank the images of\nthe training data, and return a sorted list of documents.\nFig. 2.\nSome document types differ only at speciﬁc regions. The letter (left)\nand memo (right) only differ at the address section.\nB. Region-based guidance\nAccounting for the possibility that a holistic CNN may\nnot take advantage of region-speciﬁc information in document\nimages, guiding CNNs to learn region-based features may aid\nﬁne-grained discrimination by isolating subtle region-speciﬁc\n\nefront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG.\n\nefront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG.",
      "relevant_chunks": 3,
      "sources": [
        "arxiv_1502.07058_2015.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "scores": [
        0.14378755540362456,
        0.12707132503683546,
        0.12707132503683546
      ],
      "contexts": [
        "e then used to rank the images of\nthe training data, and return a sorted list of documents.\nFig. 2.\nSome document types differ only at speciﬁc regions. The letter (left)\nand memo (right) only differ at the address section.\nB. Region-based guidance\nAccounting for the possibility that a holistic CNN may\nnot take advantage of region-speciﬁc information in document\nimages, guiding CNNs to learn region-based features may aid\nﬁne-grained discrimination by isolating subtle region-speciﬁc",
        "efront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG.",
        "efront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG."
      ],
      "execution_time": 0.5133213996887207,
      "category": "definition",
      "query_id": 3,
      "difficulty": "medium"
    },
    {
      "question": "What is the purpose of the retrieval component in RAG?",
      "answer": "uestion answering depicted in\nFigure 1, adapted from Tellex et al. [20]. Although details vary from\nsystem to system, a general QA architecture consists of a question\nanalysis component to convert the natural language question into\na search query, a document retrieval component to fetch a set of\ndocuments, and an answer selection component to identify the best\nsentences (or more generally, passages). In some designs, an answer\nextraction component identiﬁes the exact natural language phrase\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
      "relevant_chunks": 3,
      "sources": [
        "arxiv_1707.07804_2017.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.265550574263856,
        0.23816899364539046,
        0.23677440368682331
      ],
      "contexts": [
        "uestion answering depicted in\nFigure 1, adapted from Tellex et al. [20]. Although details vary from\nsystem to system, a general QA architecture consists of a question\nanalysis component to convert the natural language question into\na search query, a document retrieval component to fetch a set of\ndocuments, and an answer selection component to identify the best\nsentences (or more generally, passages). In some designs, an answer\nextraction component identiﬁes the exact natural language phrase",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco."
      ],
      "execution_time": 0.5167930126190186,
      "category": "definition",
      "query_id": 4,
      "difficulty": "easy"
    },
    {
      "question": "Explain the concept of grounding in RAG systems",
      "answer": "to represent\nthe 3D bounding boxes prediction when the task necessi-\ntates 3D bounding box outputs. The last layer embedding of\nthis location token is then sent to the 3D grounding decoder\nas a condition. Grounding Decoder receives both the 3D\npatch features and the obtained location token embeddings\nas inputs and predicts the 3D visual grounding results.\nC.2. Grounding Decoder Details\nHere illustrate more architectural and training objective de-\ntails about the grounding decoder.\n\nfor the LLM, 3D position encoding layer\nand grounding decoder, and 2e-6 for the vision encoder. The\ntraining objectives consist of the auto-regressive language\nmodeling loss and the grounding decoder training loss.\nSettings of Stage 2. In stage 2, we freeze all the components\nexcept for the grounding decoder. The model undergoes 40\ntraining epochs on 16 A100 GPUs with a peak learning rate\nof 1e-4.\nB. Training Convergence Speed\nTo further validate the effectiveness of 2D LMM-based\n\nires 3D bounding box outputs, the ground-\ning decoder will be trained together. This training setup\nensures that LLaVA-3D can effectively process both 2D and\n3D visual tokens and is adaptive to various tasks.\nStage 2: Decoder-only Fine-tuning. Since the grounding\ndecoder fails to converge within a single epoch during the\nfirst stage of training, we further train the grounding de-\ncoder for additional epochs using the 3D visual grounding\ndata while keeping all other components frozen.",
      "relevant_chunks": 3,
      "sources": [
        "2409.18125.pdf",
        "2409.18125.pdf",
        "2409.18125.pdf"
      ],
      "scores": [
        0.2535408438297368,
        0.1928893693277998,
        0.18871913874727397
      ],
      "contexts": [
        "to represent\nthe 3D bounding boxes prediction when the task necessi-\ntates 3D bounding box outputs. The last layer embedding of\nthis location token is then sent to the 3D grounding decoder\nas a condition. Grounding Decoder receives both the 3D\npatch features and the obtained location token embeddings\nas inputs and predicts the 3D visual grounding results.\nC.2. Grounding Decoder Details\nHere illustrate more architectural and training objective de-\ntails about the grounding decoder.",
        "for the LLM, 3D position encoding layer\nand grounding decoder, and 2e-6 for the vision encoder. The\ntraining objectives consist of the auto-regressive language\nmodeling loss and the grounding decoder training loss.\nSettings of Stage 2. In stage 2, we freeze all the components\nexcept for the grounding decoder. The model undergoes 40\ntraining epochs on 16 A100 GPUs with a peak learning rate\nof 1e-4.\nB. Training Convergence Speed\nTo further validate the effectiveness of 2D LMM-based",
        "ires 3D bounding box outputs, the ground-\ning decoder will be trained together. This training setup\nensures that LLaVA-3D can effectively process both 2D and\n3D visual tokens and is adaptive to various tasks.\nStage 2: Decoder-only Fine-tuning. Since the grounding\ndecoder fails to converge within a single epoch during the\nfirst stage of training, we further train the grounding de-\ncoder for additional epochs using the 3D visual grounding\ndata while keeping all other components frozen."
      ],
      "execution_time": 0.5230083465576172,
      "category": "definition",
      "query_id": 5,
      "difficulty": "medium"
    },
    {
      "question": "How does the retrieval mechanism work in RAG systems?",
      "answer": "to the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\n\nassociative\nmatrix (Schlag et al., 2020). This approach further allows us to cast the memory update\nand retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage\nstable training techniques from the related methods. Specially, we adopt the update rule\nand retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and\ncompetitive performance.\nMemory retrieval. In Infini-attention, we retrieve new content Amem ∈IRN×dvalue from the\n\nl matching score for the triple. We\nshow that the above models are special cases of this mechanism.\nTherefore, it uniﬁes those models and lets us compare them di-\nrectly. The mechanism also enables us to develop new models\nby extending to additional embedding vectors.\nIn this paper, our contributions include the following.\n• We introduce a multi-embedding interaction mechanism\nas a new approach to unifying and generalizing a class of\nstate-of-the-art knowledge graph embedding models.",
      "relevant_chunks": 3,
      "sources": [
        "2401.05856.pdf",
        "2404.07143.pdf",
        "arxiv_1903.11406_2019.pdf"
      ],
      "scores": [
        0.18692839742107134,
        0.1788873039122514,
        0.15463107350735347
      ],
      "contexts": [
        "to the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using",
        "associative\nmatrix (Schlag et al., 2020). This approach further allows us to cast the memory update\nand retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage\nstable training techniques from the related methods. Specially, we adopt the update rule\nand retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and\ncompetitive performance.\nMemory retrieval. In Infini-attention, we retrieve new content Amem ∈IRN×dvalue from the",
        "l matching score for the triple. We\nshow that the above models are special cases of this mechanism.\nTherefore, it uniﬁes those models and lets us compare them di-\nrectly. The mechanism also enables us to develop new models\nby extending to additional embedding vectors.\nIn this paper, our contributions include the following.\n• We introduce a multi-embedding interaction mechanism\nas a new approach to unifying and generalizing a class of\nstate-of-the-art knowledge graph embedding models."
      ],
      "execution_time": 0.5170109272003174,
      "category": "technical",
      "query_id": 6,
      "difficulty": "medium"
    },
    {
      "question": "What is the role of embeddings in RAG?",
      "answer": "l Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.4032336071981358,
        0.38494543309846585,
        0.2847592870130249
      ],
      "contexts": [
        "l Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco."
      ],
      "execution_time": 0.5152266025543213,
      "category": "technical",
      "query_id": 7,
      "difficulty": "medium"
    },
    {
      "question": "Explain how dense retrieval works in RAG",
      "answer": "re\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\ndense representations to retrieve relevant documents more effectively.\n\ns limitation, BM25 is still widely used because of its simplicity and efficiency. BM25 is effective\nfor tasks involving simpler, keyword-based queries, although more modern retrieval models like DPR tend\nto outperform it in semantically complex tasks.\n2.2.2 Dense Passage Retrieval (DPR)\nDense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modern\napproach to information retrieval. It uses a dense vector space in which both the query and the\n\nmechanisms that adapt to changing query patterns and content requirements. This includes developing\nmodels that can dynamically update their retrieval strategies based on new information and evolving user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.",
      "relevant_chunks": 3,
      "sources": [
        "2410.12837.pdf",
        "2410.12837.pdf",
        "2410.12837.pdf"
      ],
      "scores": [
        0.26143912411401243,
        0.1845220877605303,
        0.18158963991382915
      ],
      "contexts": [
        "re\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\ndense representations to retrieve relevant documents more effectively.",
        "s limitation, BM25 is still widely used because of its simplicity and efficiency. BM25 is effective\nfor tasks involving simpler, keyword-based queries, although more modern retrieval models like DPR tend\nto outperform it in semantically complex tasks.\n2.2.2 Dense Passage Retrieval (DPR)\nDense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modern\napproach to information retrieval. It uses a dense vector space in which both the query and the",
        "mechanisms that adapt to changing query patterns and content requirements. This includes developing\nmodels that can dynamically update their retrieval strategies based on new information and evolving user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks."
      ],
      "execution_time": 0.5079455375671387,
      "category": "technical",
      "query_id": 8,
      "difficulty": "hard"
    },
    {
      "question": "What is the difference between sparse and dense retrieval?",
      "answer": "Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse\n\nRetrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse\n\nre\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\ndense representations to retrieve relevant documents more effectively.",
      "relevant_chunks": 3,
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "2410.12837.pdf"
      ],
      "scores": [
        0.29382324695795825,
        0.29382324695795825,
        0.23686011531025888
      ],
      "contexts": [
        "Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse",
        "Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse",
        "re\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\ndense representations to retrieve relevant documents more effectively."
      ],
      "execution_time": 0.5101687908172607,
      "category": "technical",
      "query_id": 9,
      "difficulty": "medium"
    },
    {
      "question": "How does chunking strategy affect RAG performance?",
      "answer": "Thus,\nchunking emerges as an indispensable step in RAG, which involves segmenting these voluminous\ndocuments into smaller, more manageable chunks to provide precise and relevant evidence for LLMs.\nAccording to actual needs, the chunking granularity ranges from documents to paragraphs, even\nsentences. However, inappropriate retrieval granularity can compromise the semantic integrity\nand affect the relevance of retrieved information [224], thereby affecting the performance of\nLLMs.\n\nND FUTURE RESEARCH\nDIRECTIONS\nThe lessons learned from the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular\n\nand affect the relevance of retrieved information [224], thereby affecting the performance of\nLLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified\nlength such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking,\nwhich is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in\ncapture structure and dependency of lengthy documents, Sarthi et al. [267] proposed RAPTOR, an",
      "relevant_chunks": 3,
      "sources": [
        "2311.05232.pdf",
        "2401.05856.pdf",
        "2311.05232.pdf"
      ],
      "scores": [
        0.2483694478255161,
        0.22104257727567683,
        0.19927886443689288
      ],
      "contexts": [
        "Thus,\nchunking emerges as an indispensable step in RAG, which involves segmenting these voluminous\ndocuments into smaller, more manageable chunks to provide precise and relevant evidence for LLMs.\nAccording to actual needs, the chunking granularity ranges from documents to paragraphs, even\nsentences. However, inappropriate retrieval granularity can compromise the semantic integrity\nand affect the relevance of retrieved information [224], thereby affecting the performance of\nLLMs.",
        "ND FUTURE RESEARCH\nDIRECTIONS\nThe lessons learned from the three case studies are shown in Table 2.\nWe present our findings for the research question: What are the\nkey considerations when engineering a RAG system? Based on our\ntakeaways we identified multiple potential research areas linked to\nRAG as follows:\n6.1\nChunking and Embeddings\nChunking documents sounds trivial. However, the quality of chunk-\ning affects the retrieval process in many ways and in particular",
        "and affect the relevance of retrieved information [224], thereby affecting the performance of\nLLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified\nlength such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking,\nwhich is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in\ncapture structure and dependency of lengthy documents, Sarthi et al. [267] proposed RAPTOR, an"
      ],
      "execution_time": 0.501389741897583,
      "category": "technical",
      "query_id": 10,
      "difficulty": "hard"
    },
    {
      "question": "What is the role of the retriever in RAG architecture?",
      "answer": "l Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.3625716440680792,
        0.34638028075318855,
        0.2567170557907451
      ],
      "contexts": [
        "l Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco."
      ],
      "execution_time": 0.5088703632354736,
      "category": "technical",
      "query_id": 41,
      "difficulty": "easy"
    },
    {
      "question": "Explain the concept of semantic search in RAG",
      "answer": "concept without asking about any\nparticular aspect of the concept (e.g. history or\nsignificance).\n4. The concept should be very specific and niche within\nthe topic of [TOPIC].\n5. The question should require a long-form response\nthat includes several specific details such as numbers,\nnames, dates, etc.\n6. Follow the question styles in the provided examples.\n7. Wrap the question in square brackets.\nTOPIC:\n[EXAMPLE TOPIC #1]\nQUESTION:\n[[EXAMPLE QUESTION #1]]\nTOPIC:\n[EXAMPLE TOPIC #2]\nQUESTION:\n\ndual fact. We allow the model to issue five (5) search queries per fact and return three (3) search\nresults per query, using the Serper API to access Google Search.20 The template for prompting the\nmodel to issue search queries is shown in Table 14—the prompt includes the search results from\nprevious search queries in order to prevent the model from issuing search queries that may result\nin duplicated information. Once all search results are gathered for a given individual fact, we ask\n\npruned by a larger τ.\nVIII. CONCLUSIONS\nIn this paper, we proposed a semantic-guided and response-\ntime-bounded graph query to search knowledge graphs ef-\nfectively and efﬁciently. We leveraged a knowledge graph\nembedding model to build the semantic graph for each query\ngraph. Then we presented an A* semantic search to ﬁnd the\ntop-k semantically similar matches from the semantic graph\naccording to the path semantic similarity. We optimized the",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "arxiv_1910.06584_2019.pdf"
      ],
      "scores": [
        0.16469941005774633,
        0.16018142016616146,
        0.1542921918761929
      ],
      "contexts": [
        "concept without asking about any\nparticular aspect of the concept (e.g. history or\nsignificance).\n4. The concept should be very specific and niche within\nthe topic of [TOPIC].\n5. The question should require a long-form response\nthat includes several specific details such as numbers,\nnames, dates, etc.\n6. Follow the question styles in the provided examples.\n7. Wrap the question in square brackets.\nTOPIC:\n[EXAMPLE TOPIC #1]\nQUESTION:\n[[EXAMPLE QUESTION #1]]\nTOPIC:\n[EXAMPLE TOPIC #2]\nQUESTION:",
        "dual fact. We allow the model to issue five (5) search queries per fact and return three (3) search\nresults per query, using the Serper API to access Google Search.20 The template for prompting the\nmodel to issue search queries is shown in Table 14—the prompt includes the search results from\nprevious search queries in order to prevent the model from issuing search queries that may result\nin duplicated information. Once all search results are gathered for a given individual fact, we ask",
        "pruned by a larger τ.\nVIII. CONCLUSIONS\nIn this paper, we proposed a semantic-guided and response-\ntime-bounded graph query to search knowledge graphs ef-\nfectively and efﬁciently. We leveraged a knowledge graph\nembedding model to build the semantic graph for each query\ngraph. Then we presented an A* semantic search to ﬁnd the\ntop-k semantically similar matches from the semantic graph\naccording to the path semantic similarity. We optimized the"
      ],
      "execution_time": 0.5108373165130615,
      "category": "technical",
      "query_id": 42,
      "difficulty": "medium"
    },
    {
      "question": "How do attention mechanisms work in RAG systems?",
      "answer": "a attention\nmechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local\nattention and long-term linear attention mechanisms in a single Transformer block.\nSuch a subtle but critical modification to the Transformer attention layer enables a natural\nextension of existing LLMs to infinitely long contexts via continual pre-training and fine-\ntuning.\nOur Infini-attention reuses all the key, value and query states of the standard attention\n\netrieved documents, either independently (pointwise) or by comparing document pairs (pairwise). RAG\nsystems utilize self-attention within the LLM to manage context and relevance across different parts of the\ninput and retrieved text. Cross-attention mechanisms are used when integrating retrieved information into\nthe generative model, ensuring that the most relevant pieces of information are emphasized during\ngeneration.\n2.3 Generator Mechanisms in RAG Systems\n\napproach is a new at-\ntention technique dubbed Infini-attention. The Infini-attention incorporates\na compressive memory into the vanilla attention mechanism and builds\nin both masked local attention and long-term linear attention mechanisms\nin a single Transformer block. We demonstrate the effectiveness of our\napproach on long-context language modeling benchmarks, 1M sequence\nlength passkey context block retrieval and 500K length book summarization\ntasks with 1B and 8B LLMs.",
      "relevant_chunks": 3,
      "sources": [
        "2404.07143.pdf",
        "2410.12837.pdf",
        "2404.07143.pdf"
      ],
      "scores": [
        0.281780194249282,
        0.23435526606209095,
        0.2170093733486572
      ],
      "contexts": [
        "a attention\nmechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local\nattention and long-term linear attention mechanisms in a single Transformer block.\nSuch a subtle but critical modification to the Transformer attention layer enables a natural\nextension of existing LLMs to infinitely long contexts via continual pre-training and fine-\ntuning.\nOur Infini-attention reuses all the key, value and query states of the standard attention",
        "etrieved documents, either independently (pointwise) or by comparing document pairs (pairwise). RAG\nsystems utilize self-attention within the LLM to manage context and relevance across different parts of the\ninput and retrieved text. Cross-attention mechanisms are used when integrating retrieved information into\nthe generative model, ensuring that the most relevant pieces of information are emphasized during\ngeneration.\n2.3 Generator Mechanisms in RAG Systems",
        "approach is a new at-\ntention technique dubbed Infini-attention. The Infini-attention incorporates\na compressive memory into the vanilla attention mechanism and builds\nin both masked local attention and long-term linear attention mechanisms\nin a single Transformer block. We demonstrate the effectiveness of our\napproach on long-context language modeling benchmarks, 1M sequence\nlength passkey context block retrieval and 500K length book summarization\ntasks with 1B and 8B LLMs."
      ],
      "execution_time": 0.5074467658996582,
      "category": "technical",
      "query_id": 43,
      "difficulty": "hard"
    },
    {
      "question": "What is the purpose of document encoding in RAG?",
      "answer": "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.\n\nry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.22848241394887414,
        0.2267876686452394,
        0.2249544673269109
      ],
      "contexts": [
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
        "ry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?"
      ],
      "execution_time": 0.5182154178619385,
      "category": "technical",
      "query_id": 44,
      "difficulty": "medium"
    },
    {
      "question": "How does RAG handle out-of-domain queries?",
      "answer": "ution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms\nwith generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs.\nThe study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated\nto handle knowledge-intensive tasks. A detailed review of the significant technological advancements in\n\ne fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.\nQuery\nPassage\nAnswer\nContext\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\n\nthematical\ndata can boost the performance of pre-trained models on key benchmarks. Akin to Li et al. (2024b), we\nperform annealing with a data mix that upsamples high-quality data in select domains. We do not include\nany training sets from commonly used benchmarks in our annealing data. This enables us to assess the true\nfew-shot learning capabilities and out-of-domain generalization of Llama 3.\nFollowing OpenAI (2023a), we evaluate the efficacy of annealing on the GSM8k (Cobbe et al.",
      "relevant_chunks": 3,
      "sources": [
        "2410.12837.pdf",
        "2311.09476.pdf",
        "2407.21783.pdf"
      ],
      "scores": [
        0.14222503504785286,
        0.13497975823088212,
        0.13341788171686841
      ],
      "contexts": [
        "ution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms\nwith generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs.\nThe study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated\nto handle knowledge-intensive tasks. A detailed review of the significant technological advancements in",
        "e fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.\nQuery\nPassage\nAnswer\nContext\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.",
        "thematical\ndata can boost the performance of pre-trained models on key benchmarks. Akin to Li et al. (2024b), we\nperform annealing with a data mix that upsamples high-quality data in select domains. We do not include\nany training sets from commonly used benchmarks in our annealing data. This enables us to assess the true\nfew-shot learning capabilities and out-of-domain generalization of Llama 3.\nFollowing OpenAI (2023a), we evaluate the efficacy of annealing on the GSM8k (Cobbe et al."
      ],
      "execution_time": 0.517798662185669,
      "category": "technical",
      "query_id": 45,
      "difficulty": "hard"
    },
    {
      "question": "What is Self-RAG and how does it differ from standard RAG?",
      "answer": "is the La Ferrassie Cave?\nWhat is the Dolmen of Menga?\nWhat is the Bronze Age Nuragic civilization?\nWhat is the Chauvet-Pont-d’Arc Cave?\nWhat is the Altamira Cave?\n63\nE.6.32\nPsychology\nWhat is the aim of The Minnesota Twin Study in the context of nature vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.20509745080205222,
        0.17643929017965337,
        0.17437938987353485
      ],
      "contexts": [
        "is the La Ferrassie Cave?\nWhat is the Dolmen of Menga?\nWhat is the Bronze Age Nuragic civilization?\nWhat is the Chauvet-Pont-d’Arc Cave?\nWhat is the Altamira Cave?\n63\nE.6.32\nPsychology\nWhat is the aim of The Minnesota Twin Study in the context of nature vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco."
      ],
      "execution_time": 0.4992361068725586,
      "category": "approaches",
      "query_id": 11,
      "difficulty": "hard"
    },
    {
      "question": "Explain the concept of Corrective RAG (CRAG)",
      "answer": "concept without asking about any\nparticular aspect of the concept (e.g. history or\nsignificance).\n4. The concept should be very specific and niche within\nthe topic of [TOPIC].\n5. The question should require a long-form response\nthat includes several specific details such as numbers,\nnames, dates, etc.\n6. Follow the question styles in the provided examples.\n7. Wrap the question in square brackets.\nTOPIC:\n[EXAMPLE TOPIC #1]\nQUESTION:\n[[EXAMPLE QUESTION #1]]\nTOPIC:\n[EXAMPLE TOPIC #2]\nQUESTION:\n\n0], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both the step-back\nquestion and the original query are used for retrieval, and both\nthe results are utilized as the basis for language model answer\ngeneration.\n3) Query Routing: Based on varying queries, routing to\ndistinct RAG pipeline,which is suitable for a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter.\n\n0], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both the step-back\nquestion and the original query are used for retrieval, and both\nthe results are utilized as the basis for language model answer\ngeneration.\n3) Query Routing: Based on varying queries, routing to\ndistinct RAG pipeline,which is suitable for a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter.",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "scores": [
        0.18480891067599028,
        0.16402807035641936,
        0.16402807035641936
      ],
      "contexts": [
        "concept without asking about any\nparticular aspect of the concept (e.g. history or\nsignificance).\n4. The concept should be very specific and niche within\nthe topic of [TOPIC].\n5. The question should require a long-form response\nthat includes several specific details such as numbers,\nnames, dates, etc.\n6. Follow the question styles in the provided examples.\n7. Wrap the question in square brackets.\nTOPIC:\n[EXAMPLE TOPIC #1]\nQUESTION:\n[[EXAMPLE QUESTION #1]]\nTOPIC:\n[EXAMPLE TOPIC #2]\nQUESTION:",
        "0], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both the step-back\nquestion and the original query are used for retrieval, and both\nthe results are utilized as the basis for language model answer\ngeneration.\n3) Query Routing: Based on varying queries, routing to\ndistinct RAG pipeline,which is suitable for a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter.",
        "0], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both the step-back\nquestion and the original query are used for retrieval, and both\nthe results are utilized as the basis for language model answer\ngeneration.\n3) Query Routing: Based on varying queries, routing to\ndistinct RAG pipeline,which is suitable for a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter."
      ],
      "execution_time": 0.5175588130950928,
      "category": "approaches",
      "query_id": 12,
      "difficulty": "hard"
    },
    {
      "question": "How does Hybrid RAG combine different retrieval methods?",
      "answer": "mechanisms that adapt to changing query patterns and content requirements. This includes developing\nmodels that can dynamically update their retrieval strategies based on new information and evolving user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\n\n(oi,yi) log oi[yi], where yi is the\nannotation label of sample i.\nInterpolation with Language Model\nVarious studies have shown that neural ranking models\nare good at capturing soft-match signals (Guo et al. 2016;\nXiong et al. 2017). However, are exact match signals still\nneeded for neural methods? We examine this hypothesis by\nadopting a commonly-used linear interpolation method to\ncombine the match scores of NN-based models with lan-\nguage model scores between a (query, doc) pair:\n\nhis\nlimitation has prompted the exploration of hybrid models that combine retrieval mechanisms with\ngenerative capabilities to ensure both fluency and factual correctness in outputs. There has been a\nsignificant rise in several research papers in this field and several new methods across the RAG\ncomponents have been proposed. Apart from new algorithms and methods, RAG has also seen steep\nadoption across various applications. However, there is a gap in a sufficient survey of this space tracking",
      "relevant_chunks": 3,
      "sources": [
        "2410.12837.pdf",
        "arxiv_1805.08159_2018.pdf",
        "2410.12837.pdf"
      ],
      "scores": [
        0.24504494767528967,
        0.17997195961267884,
        0.1613376498164293
      ],
      "contexts": [
        "mechanisms that adapt to changing query patterns and content requirements. This includes developing\nmodels that can dynamically update their retrieval strategies based on new information and evolving user\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.",
        "(oi,yi) log oi[yi], where yi is the\nannotation label of sample i.\nInterpolation with Language Model\nVarious studies have shown that neural ranking models\nare good at capturing soft-match signals (Guo et al. 2016;\nXiong et al. 2017). However, are exact match signals still\nneeded for neural methods? We examine this hypothesis by\nadopting a commonly-used linear interpolation method to\ncombine the match scores of NN-based models with lan-\nguage model scores between a (query, doc) pair:",
        "his\nlimitation has prompted the exploration of hybrid models that combine retrieval mechanisms with\ngenerative capabilities to ensure both fluency and factual correctness in outputs. There has been a\nsignificant rise in several research papers in this field and several new methods across the RAG\ncomponents have been proposed. Apart from new algorithms and methods, RAG has also seen steep\nadoption across various applications. However, there is a gap in a sufficient survey of this space tracking"
      ],
      "execution_time": 0.5140600204467773,
      "category": "approaches",
      "query_id": 13,
      "difficulty": "medium"
    },
    {
      "question": "What is the advantage of query rewriting in RAG?",
      "answer": "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nxed\nstructures of Naive and Advanced RAG, characterized by a\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM’s capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model.\n\nxed\nstructures of Naive and Advanced RAG, characterized by a\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM’s capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model.",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "scores": [
        0.21978301320262736,
        0.21925282289529482,
        0.21925282289529482
      ],
      "contexts": [
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "xed\nstructures of Naive and Advanced RAG, characterized by a\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM’s capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model.",
        "xed\nstructures of Naive and Advanced RAG, characterized by a\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM’s capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model."
      ],
      "execution_time": 0.5114266872406006,
      "category": "approaches",
      "query_id": 14,
      "difficulty": "medium"
    },
    {
      "question": "Explain the re-ranking stage in Advanced RAG",
      "answer": "performance of our model in both basic Chinese language tasks\nand advanced Chinese reasoning tasks. Besides, we can find that the DPO process has brought\nimprovements in almost all fields.\nafter stage-2 tuning, while maintaining the benchmark score. In the case of the 67B model, the\nrepetition ratio is already below 1% following the first stage fine-tuning, and the second stage\nhurts the model score on the benchmark. Therefore, only one stage of SFT is done for the 67B\nmodel.\n5.2.\n\nrize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2.\n\nrize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2.",
      "relevant_chunks": 3,
      "sources": [
        "2401.02954.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "scores": [
        0.18328861480126668,
        0.1719882972750876,
        0.1719882972750876
      ],
      "contexts": [
        "performance of our model in both basic Chinese language tasks\nand advanced Chinese reasoning tasks. Besides, we can find that the DPO process has brought\nimprovements in almost all fields.\nafter stage-2 tuning, while maintaining the benchmark score. In the case of the 67B model, the\nrepetition ratio is already below 1% following the first stage fine-tuning, and the second stage\nhurts the model score on the benchmark. Therefore, only one stage of SFT is done for the 67B\nmodel.\n5.2.",
        "rize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2.",
        "rize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2."
      ],
      "execution_time": 0.5092432498931885,
      "category": "approaches",
      "query_id": 15,
      "difficulty": "medium"
    },
    {
      "question": "What is iterative RAG and when is it beneficial?",
      "answer": "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.28266358381468415,
        0.282136164266279,
        0.2821177566054811
      ],
      "contexts": [
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "ry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?"
      ],
      "execution_time": 0.563399076461792,
      "category": "approaches",
      "query_id": 46,
      "difficulty": "hard"
    },
    {
      "question": "Explain the concept of fusion in hybrid RAG systems",
      "answer": "rate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems\n\nhitectures (Vaswani et al. 2017), offered fluency and creativity but often lacked factual\naccuracy.\nThe development of hybrid systems combining retrieval and generation began to gain momentum as\nresearchers recognized the complementary strengths of both approaches. Early efforts in hybrid modeling\ncan be traced back to works like DrQA (Chen et al. 2017), which employed retrieval techniques to fetch\nrelevant documents for question-answering tasks.\n\nnt research in this area, current challenges and limitations of RAG, and\nfuture research direction.\n2: Core Components and Architectural Overview of RAG Systems\n2.1 Overview of RAG Models\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural\nlanguage generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base.\nTraditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast",
      "relevant_chunks": 3,
      "sources": [
        "2401.05856.pdf",
        "2410.12837.pdf",
        "2410.12837.pdf"
      ],
      "scores": [
        0.15423835229218064,
        0.14775071497655132,
        0.13601026295596394
      ],
      "contexts": [
        "rate an answer from the included\ncontext. This facilitates continuously updating the knowledge with\nnew documents and also gives the control over what chunks the user\nis able to access. However, optimal strategies for chunk embedding,\nretrieval, and contextual fusion remain active research. Further\nwork should systematically compare finetuning and RAG paradigms\nacross factors including accuracy, latency, operating costs, and\nrobustness.\n6.3\nTesting and Monitoring RAG systems",
        "hitectures (Vaswani et al. 2017), offered fluency and creativity but often lacked factual\naccuracy.\nThe development of hybrid systems combining retrieval and generation began to gain momentum as\nresearchers recognized the complementary strengths of both approaches. Early efforts in hybrid modeling\ncan be traced back to works like DrQA (Chen et al. 2017), which employed retrieval techniques to fetch\nrelevant documents for question-answering tasks.",
        "nt research in this area, current challenges and limitations of RAG, and\nfuture research direction.\n2: Core Components and Architectural Overview of RAG Systems\n2.1 Overview of RAG Models\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural\nlanguage generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base.\nTraditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast"
      ],
      "execution_time": 0.6947836875915527,
      "category": "approaches",
      "query_id": 47,
      "difficulty": "medium"
    },
    {
      "question": "What is adaptive retrieval in Self-RAG?",
      "answer": "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.\n\nry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.31601496785074645,
        0.3126963707256049,
        0.31005326526697635
      ],
      "contexts": [
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
        "ry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?"
      ],
      "execution_time": 0.550377607345581,
      "category": "approaches",
      "query_id": 48,
      "difficulty": "hard"
    },
    {
      "question": "How does context enrichment improve RAG performance?",
      "answer": "Data management in systems biology II– Outlook towards the semantic web \nGerhard Mayer, University of Stuttgart, Institute of Biochemical Engineering (IBVT), Allmandring 31, D-70569 \nStuttgart \n \nAbstract \nThe benefit of using ontologies, defined by the respective data standards, is shown. It is presented how \nontologies can be used for the semantic enrichment of data and how this can contribute to the vision of the \nsemantic web to become true.\n\nic enrichment of data and how this can contribute to the vision of the \nsemantic web to become true. The problems existing today on the way to a true semantic web are pinpointed, \ndifferent semantic web standards, tools and development frameworks are overlooked and an outlook towards \nartificial intelligence and agents for searching and mining the data in the semantic web are given, paving the \nway from data management to information and in the end true knowledge management systems.\n\nts (enabled by the hierarchical archi-\ntecture), are crucial to our model’s effectiveness. However,\nwe still lack knowledge about the following two questions:\n(1) What are the common characteristics of well-performing\ntopics, and how do the different components contribute to\noverall effectiveness? (2) When does our model fail, and\nhow can we further improve the model? To answer these\nquestions, we provide additional qualitative and quantita-",
      "relevant_chunks": 3,
      "sources": [
        "0912.2822.pdf",
        "0912.2822.pdf",
        "arxiv_1805.08159_2018.pdf"
      ],
      "scores": [
        0.175540938705963,
        0.1488580618068598,
        0.13029945311627333
      ],
      "contexts": [
        "Data management in systems biology II– Outlook towards the semantic web \nGerhard Mayer, University of Stuttgart, Institute of Biochemical Engineering (IBVT), Allmandring 31, D-70569 \nStuttgart \n \nAbstract \nThe benefit of using ontologies, defined by the respective data standards, is shown. It is presented how \nontologies can be used for the semantic enrichment of data and how this can contribute to the vision of the \nsemantic web to become true.",
        "ic enrichment of data and how this can contribute to the vision of the \nsemantic web to become true. The problems existing today on the way to a true semantic web are pinpointed, \ndifferent semantic web standards, tools and development frameworks are overlooked and an outlook towards \nartificial intelligence and agents for searching and mining the data in the semantic web are given, paving the \nway from data management to information and in the end true knowledge management systems.",
        "ts (enabled by the hierarchical archi-\ntecture), are crucial to our model’s effectiveness. However,\nwe still lack knowledge about the following two questions:\n(1) What are the common characteristics of well-performing\ntopics, and how do the different components contribute to\noverall effectiveness? (2) When does our model fail, and\nhow can we further improve the model? To answer these\nquestions, we provide additional qualitative and quantita-"
      ],
      "execution_time": 0.5265014171600342,
      "category": "approaches",
      "query_id": 49,
      "difficulty": "medium"
    },
    {
      "question": "What is the role of web search in Corrective RAG?",
      "answer": "l Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nntic annotation of data like ontologies became \nmore and more popular. Even if the semantic web is \nstill in its infancy, a possible way from web 2.0 \ntowards web 3.0 and web 4.0 applications based on \nthe W3C semantic web model is described. It is \nshown that the today existing semantic web \nstandards and tools are not yet mature and \nconsolidated enough to quickly become reality, but \nthat a smooth transition towards a real semantic web \ncan be reached. \n \nONTOLOGIES",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "0912.2822.pdf"
      ],
      "scores": [
        0.36844931923631763,
        0.3517387445798068,
        0.2835058968115226
      ],
      "contexts": [
        "l Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\nWhat can you tell me about Elon Musk’s role at Tesla?\nWho is Jack Welch and what was his role at General Electric?\nWho is Indra Nooyi?\nCan you provide information about Marillyn Hewson’s tenure as CEO of Lockheed Martin?\nWho is Jim Hackett and what was his role at Ford Motor Company?\nE.6.23\nMarketing\nCan you explain the viral component of ALS Association’s \"Ice Bucket Challenge\" marketing",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "ntic annotation of data like ontologies became \nmore and more popular. Even if the semantic web is \nstill in its infancy, a possible way from web 2.0 \ntowards web 3.0 and web 4.0 applications based on \nthe W3C semantic web model is described. It is \nshown that the today existing semantic web \nstandards and tools are not yet mature and \nconsolidated enough to quickly become reality, but \nthat a smooth transition towards a real semantic web \ncan be reached. \n \nONTOLOGIES"
      ],
      "execution_time": 0.512451171875,
      "category": "approaches",
      "query_id": 50,
      "difficulty": "medium"
    },
    {
      "question": "What metrics should be used to evaluate RAG systems?",
      "answer": "be dealt with in the\nfollowing way. The full target should be substituted into the question in the\ncorrect place as one of the Gold Standards. The target without the bracketed\nword and with it should also be included in the Gold Standard.\n7. Stemming and Synonyms: Words should not be stemmed and synonyms\nshould not be used unless they are found in the target or the current question\nseries. If they are found then both should be used in the Gold Standard.\n8.\n\nude a reference to a\nprevious question should be reformulated to include a PREVIOUS ANSWER\nvariable. Another reformulation should also be provided should a system\nknow it needs the answer to the previous question but has not found one.\nThis should be a reformulation of the previous question within the current\nquestion.\n6\n6. Targets that contain brackets: Brackets in target should be dealt with in the\nfollowing way. The full target should be substituted into the question in the\n\nSimMetrics library as it is an open source extensible library of Similarity or\nDistance Metrics 1.\nThe next task was to run the metrics provided by SimMetrics over the three\ndatasets and look to see which of the metrics score as we would expect over these\ndatasets. From these results we found that certain metrics were not appropriate.\nSome of them did the opposite to what we required them to do, in that they scored\na reformulation without the target higher than one with the target.",
      "relevant_chunks": 3,
      "sources": [
        "arxiv_1304.7157_2013.pdf",
        "arxiv_1304.7157_2013.pdf",
        "arxiv_1304.7157_2013.pdf"
      ],
      "scores": [
        0.24156157459685587,
        0.23767197212993496,
        0.2332085516816325
      ],
      "contexts": [
        "be dealt with in the\nfollowing way. The full target should be substituted into the question in the\ncorrect place as one of the Gold Standards. The target without the bracketed\nword and with it should also be included in the Gold Standard.\n7. Stemming and Synonyms: Words should not be stemmed and synonyms\nshould not be used unless they are found in the target or the current question\nseries. If they are found then both should be used in the Gold Standard.\n8.",
        "ude a reference to a\nprevious question should be reformulated to include a PREVIOUS ANSWER\nvariable. Another reformulation should also be provided should a system\nknow it needs the answer to the previous question but has not found one.\nThis should be a reformulation of the previous question within the current\nquestion.\n6\n6. Targets that contain brackets: Brackets in target should be dealt with in the\nfollowing way. The full target should be substituted into the question in the",
        "SimMetrics library as it is an open source extensible library of Similarity or\nDistance Metrics 1.\nThe next task was to run the metrics provided by SimMetrics over the three\ndatasets and look to see which of the metrics score as we would expect over these\ndatasets. From these results we found that certain metrics were not appropriate.\nSome of them did the opposite to what we required them to do, in that they scored\na reformulation without the target higher than one with the target."
      ],
      "execution_time": 0.505831241607666,
      "category": "evaluation",
      "query_id": 16,
      "difficulty": "medium"
    },
    {
      "question": "How do you measure faithfulness in RAG outputs?",
      "answer": "et al., 2023c,b).\n32\nInstructions:\n1. You have been given a STATEMENT and some KNOWLEDGE points.\n2. Your goal is to try to find evidence that either supports or does not support the factual accuracy of\nthe given STATEMENT.\n3. To do this, you are allowed to issue ONE Google Search query that you think will allow you to\nfind additional useful evidence.\n4. Your query should aim to obtain new information that does not appear in the KNOWLEDGE. This\n\nInspired by the above observations, an idea to improve SAFE is to add an option for the LLM to stop\ngenerating more queries when it determines that the collected evidence is enough to judge whether the\nclaim is supported. Right now we instruct the LLM to issue five search queries using the instruction\n“To do this, you are allowed to issue ONE Google Search query that you think will allow you to\nfind additional useful evidence” (Table 14). We do this regardless of whether the already-collected\n\nhand mirror under the garbage disposable machine\nTable 37 | An example of PIQA.\n46\nPROMPT\nArticle:\nWhen you read an article you will understand and remember it better if you can work out\nhow the writer has put the ideas together. Sometimes a writer puts ideas together by asking\nquestions and then answering them. For example, if the article is about groundhogs, the set\nof questions in the writer’s head might be:\nWhat does a groundhog look like?\nWhere do groundhogs live?\nWhat do they eat?...",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2401.02954.pdf"
      ],
      "scores": [
        0.20635272366398133,
        0.17798452090802183,
        0.17373322703694205
      ],
      "contexts": [
        "et al., 2023c,b).\n32\nInstructions:\n1. You have been given a STATEMENT and some KNOWLEDGE points.\n2. Your goal is to try to find evidence that either supports or does not support the factual accuracy of\nthe given STATEMENT.\n3. To do this, you are allowed to issue ONE Google Search query that you think will allow you to\nfind additional useful evidence.\n4. Your query should aim to obtain new information that does not appear in the KNOWLEDGE. This",
        "Inspired by the above observations, an idea to improve SAFE is to add an option for the LLM to stop\ngenerating more queries when it determines that the collected evidence is enough to judge whether the\nclaim is supported. Right now we instruct the LLM to issue five search queries using the instruction\n“To do this, you are allowed to issue ONE Google Search query that you think will allow you to\nfind additional useful evidence” (Table 14). We do this regardless of whether the already-collected",
        "hand mirror under the garbage disposable machine\nTable 37 | An example of PIQA.\n46\nPROMPT\nArticle:\nWhen you read an article you will understand and remember it better if you can work out\nhow the writer has put the ideas together. Sometimes a writer puts ideas together by asking\nquestions and then answering them. For example, if the article is about groundhogs, the set\nof questions in the writer’s head might be:\nWhat does a groundhog look like?\nWhere do groundhogs live?\nWhat do they eat?..."
      ],
      "execution_time": 0.5094113349914551,
      "category": "evaluation",
      "query_id": 17,
      "difficulty": "hard"
    },
    {
      "question": "What is RAGAS and how is it used for RAG evaluation?",
      "answer": "is the La Ferrassie Cave?\nWhat is the Dolmen of Menga?\nWhat is the Bronze Age Nuragic civilization?\nWhat is the Chauvet-Pont-d’Arc Cave?\nWhat is the Altamira Cave?\n63\nE.6.32\nPsychology\nWhat is the aim of The Minnesota Twin Study in the context of nature vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\n\nry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.251531533739867,
        0.23872115785739348,
        0.23867463809722134
      ],
      "contexts": [
        "is the La Ferrassie Cave?\nWhat is the Dolmen of Menga?\nWhat is the Bronze Age Nuragic civilization?\nWhat is the Chauvet-Pont-d’Arc Cave?\nWhat is the Altamira Cave?\n63\nE.6.32\nPsychology\nWhat is the aim of The Minnesota Twin Study in the context of nature vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?",
        "ry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?"
      ],
      "execution_time": 0.5122425556182861,
      "category": "evaluation",
      "query_id": 18,
      "difficulty": "hard"
    },
    {
      "question": "Explain the difference between retrieval and generation metrics",
      "answer": "them to do, in that they scored\na reformulation without the target higher than one with the target. This could be\ndue to over-emphasis on word ordering. These metrics were discounted at this\nstage. Other metrics were also discounted as the difference between “With Target”\nand “Without Target” was not large enough; it would have been difﬁcult to measure\nimprovements in the system with a small difference.\nThe four best performing metrics were: DiceSimilarity, JaccardSimilarity, Block-\n\nalues for 30 answer pairs. For each\npair, the value in X is the difference between the two answers’\nranks given by SGQ, and the value in Y is the difference\nbetween the numbers of annotators favoring the two answers.\nThen, we calculated the PCC for each query based on Eq.\n10. The PCC value shows the degree of correlation between\nthe preference given by SGQ and annotators. A PCC value\nin the ranges of [0.5,1.0], [0.3,0.5) and [0.1,0.3) indicates a\n\nmain evaluation objectives include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality.",
      "relevant_chunks": 3,
      "sources": [
        "arxiv_1304.7157_2013.pdf",
        "arxiv_1910.06584_2019.pdf",
        "2312.10997.pdf"
      ],
      "scores": [
        0.19739214584780101,
        0.17679750161173155,
        0.1767119418587431
      ],
      "contexts": [
        "them to do, in that they scored\na reformulation without the target higher than one with the target. This could be\ndue to over-emphasis on word ordering. These metrics were discounted at this\nstage. Other metrics were also discounted as the difference between “With Target”\nand “Without Target” was not large enough; it would have been difﬁcult to measure\nimprovements in the system with a small difference.\nThe four best performing metrics were: DiceSimilarity, JaccardSimilarity, Block-",
        "alues for 30 answer pairs. For each\npair, the value in X is the difference between the two answers’\nranks given by SGQ, and the value in Y is the difference\nbetween the numbers of annotators favoring the two answers.\nThen, we calculated the PCC for each query based on Eq.\n10. The PCC value shows the degree of correlation between\nthe preference given by SGQ and annotators. A PCC value\nin the ranges of [0.5,1.0], [0.3,0.5) and [0.1,0.3) indicates a",
        "main evaluation objectives include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality."
      ],
      "execution_time": 0.5141916275024414,
      "category": "evaluation",
      "query_id": 19,
      "difficulty": "medium"
    },
    {
      "question": "How can we detect hallucinations in RAG systems?",
      "answer": "Ms\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present considerable complexities, manifesting\nas outputs that are either factually inaccurate or misleading. These hallucinations occur when\nthe content generated by the LLM does not align with real-world facts, fails to accurately reflect\nthe user’s query, or is not supported by the retrieved information. Such hallucinations can stem\n\nLM\nhallucinations in depth (§3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (§6) and delineate potential pathways for forthcoming research (§7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a\n\ne present a comprehensive review of contemporary methods aimed at mitigating\nhallucinations in LLMs. Drawing from insights discussed in Hallucination Causes (§3), we systemat-\nically categorize these methods based on the underlying causes of hallucinations. Specifically, we\nfocus on approaches addressing Data-related Hallucinations (§5.1), Training-related Hallucinations\n(§5.2) and Inference-related Hallucinations (§5.3), each offering tailored solutions to tackle specific",
      "relevant_chunks": 3,
      "sources": [
        "2311.05232.pdf",
        "2311.05232.pdf",
        "2311.05232.pdf"
      ],
      "scores": [
        0.26054854479886796,
        0.23555881824743322,
        0.20925555130694284
      ],
      "contexts": [
        "Ms\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present considerable complexities, manifesting\nas outputs that are either factually inaccurate or misleading. These hallucinations occur when\nthe content generated by the LLM does not align with real-world facts, fails to accurately reflect\nthe user’s query, or is not supported by the retrieved information. Such hallucinations can stem",
        "LM\nhallucinations in depth (§3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (§6) and delineate potential pathways for forthcoming research (§7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a",
        "e present a comprehensive review of contemporary methods aimed at mitigating\nhallucinations in LLMs. Drawing from insights discussed in Hallucination Causes (§3), we systemat-\nically categorize these methods based on the underlying causes of hallucinations. Specifically, we\nfocus on approaches addressing Data-related Hallucinations (§5.1), Training-related Hallucinations\n(§5.2) and Inference-related Hallucinations (§5.3), each offering tailored solutions to tackle specific"
      ],
      "execution_time": 0.5090296268463135,
      "category": "evaluation",
      "query_id": 20,
      "difficulty": "hard"
    },
    {
      "question": "What is the Lost in the Middle problem in RAG?",
      "answer": "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.\n\neds,\nthousands or tens of thousands. The topics were\nchosen without knowledge of any experiment re-\nsults related to them, such that they were not picked\nto achieve a particular outcome.\n(†)\n1. Animals + Brain + Rats.\n2. Adult + Middle Aged + HIV Infections.\n3. Lymphatic Metastasis + Middle Aged + Neo-\nplasm Staging.\n4. Base Sequence + Molecular Sequence Data +\nPromoter Regions, Genetic.\n5. Renal Dialysis + Kidney Failure, Chronic +\nMiddle Aged.\n6. Aged + Middle Aged + Laparoscopy.\n7.",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "arxiv_1910.13339_2019.pdf"
      ],
      "scores": [
        0.20219431945693203,
        0.2007042041208373,
        0.19888820053632056
      ],
      "contexts": [
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
        "eds,\nthousands or tens of thousands. The topics were\nchosen without knowledge of any experiment re-\nsults related to them, such that they were not picked\nto achieve a particular outcome.\n(†)\n1. Animals + Brain + Rats.\n2. Adult + Middle Aged + HIV Infections.\n3. Lymphatic Metastasis + Middle Aged + Neo-\nplasm Staging.\n4. Base Sequence + Molecular Sequence Data +\nPromoter Regions, Genetic.\n5. Renal Dialysis + Kidney Failure, Chronic +\nMiddle Aged.\n6. Aged + Middle Aged + Laparoscopy.\n7."
      ],
      "execution_time": 0.5129539966583252,
      "category": "challenges",
      "query_id": 21,
      "difficulty": "hard"
    },
    {
      "question": "How does context length affect RAG performance?",
      "answer": "2024).\nThe passkey task hides a random number into a long text and asks it back at the model\noutput. The length of the distraction text is varied by repeating a text chunk multiple times.\nThe previous work (Chen et al., 2023a) showed that a 8B LLaMA model can solve the task up\nto 32K length when fine-tuned with the same 32K length inputs with Position Interpolation.\nWe take this challenge further and fine-tune on only 5K length inputs to test on 1M length\nregime.\nInput length\n\ng recall when measuring long-form factuality may better match human notions\nof long-form factuality.14\nA.7\nHow does the prompt postamble affect model responses?\nSection 6 evaluated language models on LongFact-Objects with a fixed postamble that asks the model\nto provide as many specific details as possible.15 Here, we discuss how the fixed postamble affects\nthe model responses. We intuitively expect that the postamble should encourage models to provide\n\nwe con-\ntrolled the position of the passkey so that it\nis either located around the beginning, mid-\ndle or the end of the input sequence. We\nreported both zero-shot accuracy and fine-\ntuning accuracy. Infini-Transformers solved\nthe task with up to 1M context length af-\nter fine-tuning on 5K length inputs for 400\nsteps.\n500K length book summarization (Book-\nSum). We further scaled our approach by\ncontinuously pre-training a 8B LLM model\nwith 8K input length for 30K steps. We then",
      "relevant_chunks": 3,
      "sources": [
        "2404.07143.pdf",
        "2403.18802.pdf",
        "2404.07143.pdf"
      ],
      "scores": [
        0.205005030395736,
        0.16635083224014108,
        0.16162742196940091
      ],
      "contexts": [
        "2024).\nThe passkey task hides a random number into a long text and asks it back at the model\noutput. The length of the distraction text is varied by repeating a text chunk multiple times.\nThe previous work (Chen et al., 2023a) showed that a 8B LLaMA model can solve the task up\nto 32K length when fine-tuned with the same 32K length inputs with Position Interpolation.\nWe take this challenge further and fine-tune on only 5K length inputs to test on 1M length\nregime.\nInput length",
        "g recall when measuring long-form factuality may better match human notions\nof long-form factuality.14\nA.7\nHow does the prompt postamble affect model responses?\nSection 6 evaluated language models on LongFact-Objects with a fixed postamble that asks the model\nto provide as many specific details as possible.15 Here, we discuss how the fixed postamble affects\nthe model responses. We intuitively expect that the postamble should encourage models to provide",
        "we con-\ntrolled the position of the passkey so that it\nis either located around the beginning, mid-\ndle or the end of the input sequence. We\nreported both zero-shot accuracy and fine-\ntuning accuracy. Infini-Transformers solved\nthe task with up to 1M context length af-\nter fine-tuning on 5K length inputs for 400\nsteps.\n500K length book summarization (Book-\nSum). We further scaled our approach by\ncontinuously pre-training a 8B LLM model\nwith 8K input length for 30K steps. We then"
      ],
      "execution_time": 0.49947571754455566,
      "category": "challenges",
      "query_id": 22,
      "difficulty": "medium"
    },
    {
      "question": "What are the common failure modes of RAG systems?",
      "answer": "a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?\n\ns/references\nto generated responses, and c) remove the need for annotating\ndocuments with meta-data. However, RAG systems suffer from lim-\nitations inherent to information retrieval systems and from reliance\non LLMs. In this paper, we present an experience report on the\nfailure points of RAG systems from three case studies from separate\ndomains: research, education, and biomedical. We share the lessons\nlearned and present 7 failure points to consider when designing a\nRAG system.\n\nto the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using",
      "relevant_chunks": 3,
      "sources": [
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2401.05856.pdf"
      ],
      "scores": [
        0.2784447737420521,
        0.1819184238544725,
        0.17848038099493174
      ],
      "contexts": [
        "a human rater for this domain.\nHowever, one threat to validity with this finding is that BioASQ is\na domain specific dataset and the reviewers were not experts i.e.\nthe large language model may know more than a non-expert.\n5\nFAILURE POINTS OF RAG SYSTEMS\nFrom the case studies we identified a set of failure points presented\nbelow. The following section addresses the research question What\nare the failure points that occur when engineering a RAG system?",
        "s/references\nto generated responses, and c) remove the need for annotating\ndocuments with meta-data. However, RAG systems suffer from lim-\nitations inherent to information retrieval systems and from reliance\non LLMs. In this paper, we present an experience report on the\nfailure points of RAG systems from three case studies from separate\ndomains: research, education, and biomedical. We share the lessons\nlearned and present 7 failure points to consider when designing a\nRAG system.",
        "to the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using"
      ],
      "execution_time": 0.5455658435821533,
      "category": "challenges",
      "query_id": 23,
      "difficulty": "hard"
    },
    {
      "question": "How can RAG systems handle multi-hop reasoning?",
      "answer": "2018; Kratzwald and Feuerriegel, 2018), but it is\ninherently limited to answering questions that do\nnot require multi-hop/multi-step reasoning. This\nis because for many multi-hop questions, not all\nthe relevant context can be obtained in a single re-\ntrieval step (e.g., “Ernest Cline” in Figure 1).\nMore recently, the emergence of multi-hop\nquestion answering datasets such as QAngaroo\n(Welbl et al., 2018) and HOTPOTQA (Yang et al.,\n2018) has sparked interest in multi-hop QA in the\n\never, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existing work by presenting challenges faced during the\nimplementation of a RAG system with a focus on practitioners.\nErrors and failures that arise from RAG systems overlap with\nother information retrieval systems including 1) no metrics for\nquery rewriting, 2) document re-ranking, and 3) effective content\nsummarisation [19]. Our results confirm this The unique aspects\n\ner,\nthese one-step retrieve-and-read approaches are\nfundamentally ill-equipped to address questions\nthat require multi-hop reasoning, especially when\nnecessary evidence is not readily retrievable with\nthe question.\nMulti-hop\nQA\ndatasets\nQAngaroo\n(Welbl\net al., 2018) and HOTPOTQA (Yang et al., 2018)\nare among the largest-scale multi-hop QA datasets\nto date. While the former is constructed around\na knowledge base and the knowledge schema\ntherein, the latter adopts a free-form question gen-",
      "relevant_chunks": 3,
      "sources": [
        "arxiv_1910.07000_2019.pdf",
        "2401.05856.pdf",
        "arxiv_1910.07000_2019.pdf"
      ],
      "scores": [
        0.1426586960817888,
        0.13006004499496512,
        0.1259301745120171
      ],
      "contexts": [
        "2018; Kratzwald and Feuerriegel, 2018), but it is\ninherently limited to answering questions that do\nnot require multi-hop/multi-step reasoning. This\nis because for many multi-hop questions, not all\nthe relevant context can be obtained in a single re-\ntrieval step (e.g., “Ernest Cline” in Figure 1).\nMore recently, the emergence of multi-hop\nquestion answering datasets such as QAngaroo\n(Welbl et al., 2018) and HOTPOTQA (Yang et al.,\n2018) has sparked interest in multi-hop QA in the",
        "ever, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\nments existing work by presenting challenges faced during the\nimplementation of a RAG system with a focus on practitioners.\nErrors and failures that arise from RAG systems overlap with\nother information retrieval systems including 1) no metrics for\nquery rewriting, 2) document re-ranking, and 3) effective content\nsummarisation [19]. Our results confirm this The unique aspects",
        "er,\nthese one-step retrieve-and-read approaches are\nfundamentally ill-equipped to address questions\nthat require multi-hop reasoning, especially when\nnecessary evidence is not readily retrievable with\nthe question.\nMulti-hop\nQA\ndatasets\nQAngaroo\n(Welbl\net al., 2018) and HOTPOTQA (Yang et al., 2018)\nare among the largest-scale multi-hop QA datasets\nto date. While the former is constructed around\na knowledge base and the knowledge schema\ntherein, the latter adopts a free-form question gen-"
      ],
      "execution_time": 0.5178825855255127,
      "category": "challenges",
      "query_id": 24,
      "difficulty": "hard"
    },
    {
      "question": "What are the scalability challenges in RAG systems?",
      "answer": "ng task-specific challenges and advancing multilingual information access for\nglobal inclusivity.\n5. Current Challenges and Limitations in Retrieval-Augmented Generation (RAG):\nThis section intends to highlight the current challenges and limitations of RAG considering the current\nlandscape of the system and this would shape the future research directions in the field.\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrieval\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
      "relevant_chunks": 3,
      "sources": [
        "2410.12837.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.3734404370399859,
        0.224538234861692,
        0.21917957915929348
      ],
      "contexts": [
        "ng task-specific challenges and advancing multilingual information access for\nglobal inclusivity.\n5. Current Challenges and Limitations in Retrieval-Augmented Generation (RAG):\nThis section intends to highlight the current challenges and limitations of RAG considering the current\nlandscape of the system and this would shape the future research directions in the field.\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrieval",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco."
      ],
      "execution_time": 0.515162467956543,
      "category": "challenges",
      "query_id": 25,
      "difficulty": "medium"
    },
    {
      "question": "What is the optimal chunk size for RAG systems?",
      "answer": "[27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference\nOnce\nPaperQA [53]\nArxiv,Online Database,PubMed\nText\nChunk\nInference\nIterative\nNoiseRAG [54]\nFactoidWiki\n\n[27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference\nOnce\nPaperQA [53]\nArxiv,Online Database,PubMed\nText\nChunk\nInference\nIterative\nNoiseRAG [54]\nFactoidWiki\n\nmaximum number of times.\n0\n1\n2\n3\n0\n10000\n20000\n30000\nNumber of self-reflection iterations performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 3: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and self-reflection iterations for our generic entity extraction prompt with\ngpt-4-turbo.\nB\nExample Community Detection\n18\n(a) Root communities at level 0\n(b) Sub-communities at level 1",
      "relevant_chunks": 3,
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "2404.16130.pdf"
      ],
      "scores": [
        0.3404197477837421,
        0.3404197477837421,
        0.3371272397855222
      ],
      "contexts": [
        "[27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference\nOnce\nPaperQA [53]\nArxiv,Online Database,PubMed\nText\nChunk\nInference\nIterative\nNoiseRAG [54]\nFactoidWiki",
        "[27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference\nOnce\nPaperQA [53]\nArxiv,Online Database,PubMed\nText\nChunk\nInference\nIterative\nNoiseRAG [54]\nFactoidWiki",
        "maximum number of times.\n0\n1\n2\n3\n0\n10000\n20000\n30000\nNumber of self-reflection iterations performed\nEntity references detected\n600 chunk size\n1200 chunk size\n2400 chunk size\nFigure 3: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and self-reflection iterations for our generic entity extraction prompt with\ngpt-4-turbo.\nB\nExample Community Detection\n18\n(a) Root communities at level 0\n(b) Sub-communities at level 1"
      ],
      "execution_time": 0.5143654346466064,
      "category": "implementation",
      "query_id": 26,
      "difficulty": "medium"
    },
    {
      "question": "How should chunk overlap be configured in RAG?",
      "answer": "[27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference\nOnce\nPaperQA [53]\nArxiv,Online Database,PubMed\nText\nChunk\nInference\nIterative\nNoiseRAG [54]\nFactoidWiki\n\n[27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference\nOnce\nPaperQA [53]\nArxiv,Online Database,PubMed\nText\nChunk\nInference\nIterative\nNoiseRAG [54]\nFactoidWiki\n\ng\nIterative\nRAVEN [43]\nWikipedia\nText\nChunk\nPre-training\nOnce\nRETRO++ [44]\nPre-training Corpus\nText\nChunk\nPre-training\nIterative\nINSTRUCTRETRO [45]\nPre-training corpus\nText\nChunk\nPre-training\nIterative\nRRR [7]\nSearch Engine\nText\nChunk\nTuning\nOnce\nRA-e2e [46]\nDataset-base\nText\nChunk\nTuning\nOnce\nPROMPTAGATOR [21]\nBEIR\nText\nChunk\nTuning\nOnce\nAAR [47]\nMSMARCO,Wikipedia\nText\nChunk\nTuning\nOnce\nRA-DIT [27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce",
      "relevant_chunks": 3,
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "2312.10997.pdf"
      ],
      "scores": [
        0.27747345231167525,
        0.27747345231167525,
        0.273408438944108
      ],
      "contexts": [
        "[27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference\nOnce\nPaperQA [53]\nArxiv,Online Database,PubMed\nText\nChunk\nInference\nIterative\nNoiseRAG [54]\nFactoidWiki",
        "[27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce\nRA-Long-Form [49]\nDataset-base\nText\nChunk\nTuning\nOnce\nCoN [50]\nWikipedia\nText\nChunk\nTuning\nOnce\nSelf-RAG [25]\nWikipedia\nText\nChunk\nTuning\nAdaptive\nBGM [26]\nWikipedia\nText\nChunk\nInference\nOnce\nCoQ [51]\nWikipedia\nText\nChunk\nInference\nIterative\nToken-Elimination [52]\nWikipedia\nText\nChunk\nInference\nOnce\nPaperQA [53]\nArxiv,Online Database,PubMed\nText\nChunk\nInference\nIterative\nNoiseRAG [54]\nFactoidWiki",
        "g\nIterative\nRAVEN [43]\nWikipedia\nText\nChunk\nPre-training\nOnce\nRETRO++ [44]\nPre-training Corpus\nText\nChunk\nPre-training\nIterative\nINSTRUCTRETRO [45]\nPre-training corpus\nText\nChunk\nPre-training\nIterative\nRRR [7]\nSearch Engine\nText\nChunk\nTuning\nOnce\nRA-e2e [46]\nDataset-base\nText\nChunk\nTuning\nOnce\nPROMPTAGATOR [21]\nBEIR\nText\nChunk\nTuning\nOnce\nAAR [47]\nMSMARCO,Wikipedia\nText\nChunk\nTuning\nOnce\nRA-DIT [27]\nCommon Crawl,Wikipedia\nText\nChunk\nTuning\nOnce\nRAG-Robust [48]\nWikipedia\nText\nChunk\nTuning\nOnce"
      ],
      "execution_time": 0.585798978805542,
      "category": "implementation",
      "query_id": 27,
      "difficulty": "medium"
    },
    {
      "question": "What embedding models are best for RAG retrieval?",
      "answer": "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.\n\nry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.19465067484673051,
        0.18910552775488582,
        0.1867374262362048
      ],
      "contexts": [
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
        "ry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?"
      ],
      "execution_time": 0.5377025604248047,
      "category": "implementation",
      "query_id": 28,
      "difficulty": "medium"
    },
    {
      "question": "How do you choose the number of retrieved documents (top-k)?",
      "answer": "t context, REPLUG first retrieves relevant\ndocuments from an external corpus using an off-the-shelf\nretrieval model. The retrieved documents are prepended to\nthe input context and fed into the black-box LM to make\nthe final prediction. Because the LM context length limits\nthe number of documents that can be prepended, we also\nintroduce a new ensemble scheme that encodes the retrieved\ndocuments in parallel with the same black-box LM, allow-\ning us to easily trade compute for accuracy.\n\net al., 2023c,b).\n32\nInstructions:\n1. You have been given a STATEMENT and some KNOWLEDGE points.\n2. Your goal is to try to find evidence that either supports or does not support the factual accuracy of\nthe given STATEMENT.\n3. To do this, you are allowed to issue ONE Google Search query that you think will allow you to\nfind additional useful evidence.\n4. Your query should aim to obtain new information that does not appear in the KNOWLEDGE. This\n\nchoose the predicted prefix with the lowest perplexity of\nthe completion.\n48",
      "relevant_chunks": 3,
      "sources": [
        "REPLUG_2023.pdf",
        "2403.18802.pdf",
        "2401.02954.pdf"
      ],
      "scores": [
        0.19808931558918116,
        0.1960278053778467,
        0.17907643973376833
      ],
      "contexts": [
        "t context, REPLUG first retrieves relevant\ndocuments from an external corpus using an off-the-shelf\nretrieval model. The retrieved documents are prepended to\nthe input context and fed into the black-box LM to make\nthe final prediction. Because the LM context length limits\nthe number of documents that can be prepended, we also\nintroduce a new ensemble scheme that encodes the retrieved\ndocuments in parallel with the same black-box LM, allow-\ning us to easily trade compute for accuracy.",
        "et al., 2023c,b).\n32\nInstructions:\n1. You have been given a STATEMENT and some KNOWLEDGE points.\n2. Your goal is to try to find evidence that either supports or does not support the factual accuracy of\nthe given STATEMENT.\n3. To do this, you are allowed to issue ONE Google Search query that you think will allow you to\nfind additional useful evidence.\n4. Your query should aim to obtain new information that does not appear in the KNOWLEDGE. This",
        "choose the predicted prefix with the lowest perplexity of\nthe completion.\n48"
      ],
      "execution_time": 0.5200793743133545,
      "category": "implementation",
      "query_id": 29,
      "difficulty": "medium"
    },
    {
      "question": "What vector databases are commonly used for RAG?",
      "answer": "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.\n\nry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?",
      "relevant_chunks": 3,
      "sources": [
        "2403.18802.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.1931441828226349,
        0.1876419522008866,
        0.18529217852028243
      ],
      "contexts": [
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
        "ry?\nWho is the chemist Roald Hoffmann, known for his contribution to computational chemistry?\nE.6.9\nClinical Knowledge\nWhat is the Cleveland Clinic?\nWhat is the Karolinska Institute?\nWhat is the University of Texas MD Anderson Cancer Center?\nE.6.10\nComputer Science\nWhat is the Deep Blue chess computer by IBM?\nWho is Edward A. Feigenbaum?\nWhat is Tesler’s Law of the Conservation of Complexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?"
      ],
      "execution_time": 0.5116438865661621,
      "category": "implementation",
      "query_id": 30,
      "difficulty": "easy"
    },
    {
      "question": "Compare BM25 and dense vector retrieval methods",
      "answer": "nal corpus.\nEffective retrieval ensures that the model's output is grounded in accurate information. Several retrieval\nmechanisms are commonly used, ranging from traditional methods like BM25 to more sophisticated\ntechniques like Dense Passage Retrieval (DPR).\n2.2.1 BM25\nBM25 is a well-established information retrieval algorithm that uses the term frequency-inverse document\nfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25\n\ns limitation, BM25 is still widely used because of its simplicity and efficiency. BM25 is effective\nfor tasks involving simpler, keyword-based queries, although more modern retrieval models like DPR tend\nto outperform it in semantically complex tasks.\n2.2.2 Dense Passage Retrieval (DPR)\nDense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modern\napproach to information retrieval. It uses a dense vector space in which both the query and the\n\nre\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\ndense representations to retrieve relevant documents more effectively.",
      "relevant_chunks": 3,
      "sources": [
        "2410.12837.pdf",
        "2410.12837.pdf",
        "2410.12837.pdf"
      ],
      "scores": [
        0.3621962058392771,
        0.348156776938322,
        0.28085478907361366
      ],
      "contexts": [
        "nal corpus.\nEffective retrieval ensures that the model's output is grounded in accurate information. Several retrieval\nmechanisms are commonly used, ranging from traditional methods like BM25 to more sophisticated\ntechniques like Dense Passage Retrieval (DPR).\n2.2.1 BM25\nBM25 is a well-established information retrieval algorithm that uses the term frequency-inverse document\nfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25",
        "s limitation, BM25 is still widely used because of its simplicity and efficiency. BM25 is effective\nfor tasks involving simpler, keyword-based queries, although more modern retrieval models like DPR tend\nto outperform it in semantically complex tasks.\n2.2.2 Dense Passage Retrieval (DPR)\nDense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modern\napproach to information retrieval. It uses a dense vector space in which both the query and the",
        "re\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\ndense representations to retrieve relevant documents more effectively."
      ],
      "execution_time": 0.6458666324615479,
      "category": "comparison",
      "query_id": 31,
      "difficulty": "medium"
    },
    {
      "question": "When should you use Naive RAG vs Advanced RAG?",
      "answer": "rize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2.\n\nrize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2.\n\nly, it incorporates several optimization\nmethods to streamline the retrieval process [8].\n4\nFig. 3.\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\nchain-like structure.",
      "relevant_chunks": 3,
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "2312.10997.pdf"
      ],
      "scores": [
        0.3780920336607144,
        0.3780920336607144,
        0.23119165399370847
      ],
      "contexts": [
        "rize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2.",
        "rize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the\n3\nFig. 2.",
        "ly, it incorporates several optimization\nmethods to streamline the retrieval process [8].\n4\nFig. 3.\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\nchain-like structure."
      ],
      "execution_time": 0.5500242710113525,
      "category": "comparison",
      "query_id": 32,
      "difficulty": "medium"
    },
    {
      "question": "Compare Self-RAG with Corrective RAG approaches",
      "answer": "open-domain question answering including the single-hop\nand multi-hop queries, with different LLMs. Self-RAG∗is trained with a different base LLM, namely LLaMA2 (Touvron et al.,\n2023); therefore, we compare the results of FLAN-T5-XL (3B) with the results from Self-RAG with LLaMA2 (7B) and the\nresults of others with the results from Self-RAG with LLaMA2 (13B). We emphasize our results in bold, for easy comparisons.\nFLAN-T5-XL (3B)\nFLAN-T5-XXL (11B)\nGPT-3.5 (Turbo)\nTypes\nMethods\nEM\nF1\nAcc\nStep\n\ndaptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\nof retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\n\ndaptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\nof retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning",
      "relevant_chunks": 3,
      "sources": [
        "Adaptive_RAG_2024.pdf",
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf"
      ],
      "scores": [
        0.23384385739944882,
        0.1781925500443143,
        0.1781925500443143
      ],
      "contexts": [
        "open-domain question answering including the single-hop\nand multi-hop queries, with different LLMs. Self-RAG∗is trained with a different base LLM, namely LLaMA2 (Touvron et al.,\n2023); therefore, we compare the results of FLAN-T5-XL (3B) with the results from Self-RAG with LLaMA2 (7B) and the\nresults of others with the results from Self-RAG with LLaMA2 (13B). We emphasize our results in bold, for easy comparisons.\nFLAN-T5-XL (3B)\nFLAN-T5-XXL (11B)\nGPT-3.5 (Turbo)\nTypes\nMethods\nEM\nF1\nAcc\nStep",
        "daptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\nof retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning",
        "daptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\nof retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning"
      ],
      "execution_time": 0.5119261741638184,
      "category": "comparison",
      "query_id": 33,
      "difficulty": "hard"
    },
    {
      "question": "What are the trade-offs between hybrid and pure dense retrieval?",
      "answer": "re\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\ndense representations to retrieve relevant documents more effectively.\n\nacross various applications. However, there is a gap in a sufficient survey of this space tracking\nthe evolution and recent changes in this space. The current survey intends to fill this gap.\n1.2 Overview of Retrieval-Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the\nlimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
      "relevant_chunks": 3,
      "sources": [
        "2410.12837.pdf",
        "2410.12837.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.13930516999356174,
        0.13800903205242,
        0.12962483275280734
      ],
      "contexts": [
        "re\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\ndense representations to retrieve relevant documents more effectively.",
        "across various applications. However, there is a gap in a sufficient survey of this space tracking\nthe evolution and recent changes in this space. The current survey intends to fill this gap.\n1.2 Overview of Retrieval-Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the\nlimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?"
      ],
      "execution_time": 0.5142498016357422,
      "category": "comparison",
      "query_id": 34,
      "difficulty": "medium"
    },
    {
      "question": "Compare single-stage vs multi-stage retrieval in RAG",
      "answer": "of the amount of labeled data. The reported\n7We note that the comparison here should be made to the\nspeciﬁc IR engine which resulted in the dataset of the PU\nmodel, as the PU model beneﬁts greatly from better perfor-\nmance in the IR engine.\n0\n500\n1000\n1500\n2000\n# of labeled samples\n0.1\n0.2\n0.3\nF1 distance\nPU vs PN, IR vs PN performance by |LP|\nvs PU\nvs IR\nFigure 1: The F1 absolute difference, normalized by\nthe sum of the two F1 scores, between the upper-bound\n\npipeline. When we combine them into a pipeline,\nthe generated queries perform only slightly better\non d1 when a total of 10 documents are retrieved\n(89.91% vs 87.85%), but are signiﬁcantly more\neffective for d2 (61.01% vs 36.91%). If we fur-\nther zoom in on the retrieval performance on non-\ncomparison questions for which ﬁnding the two\nentities involved is less trivial, we can see that the\nrecall on d2 improves from 27.88% to 53.23%, al-\nmost doubling the number of questions we have\n\ng the off-the-shelf 3D segmentor and then\nconvert the task into a 3D object selection task. Without\nrelying on the constructed point cloud, our method could\ndirectly decode the accurate 3D bounding boxes from 3D\npatches and achieve the SOTA performance (49.8 Acc@0.25\non Multi3DRefer) in the single-stage manner.\n5.4. Evaluation on 2D benchmarks\nSince our model is trained on the joint 2D and 3D datasets,\nwe evaluate it on the 2D video benchmarks to ensure it",
      "relevant_chunks": 3,
      "sources": [
        "arxiv_1910.13339_2019.pdf",
        "arxiv_1910.07000_2019.pdf",
        "2409.18125.pdf"
      ],
      "scores": [
        0.17816668080049242,
        0.12034679833448211,
        0.11021259000797665
      ],
      "contexts": [
        "of the amount of labeled data. The reported\n7We note that the comparison here should be made to the\nspeciﬁc IR engine which resulted in the dataset of the PU\nmodel, as the PU model beneﬁts greatly from better perfor-\nmance in the IR engine.\n0\n500\n1000\n1500\n2000\n# of labeled samples\n0.1\n0.2\n0.3\nF1 distance\nPU vs PN, IR vs PN performance by |LP|\nvs PU\nvs IR\nFigure 1: The F1 absolute difference, normalized by\nthe sum of the two F1 scores, between the upper-bound",
        "pipeline. When we combine them into a pipeline,\nthe generated queries perform only slightly better\non d1 when a total of 10 documents are retrieved\n(89.91% vs 87.85%), but are signiﬁcantly more\neffective for d2 (61.01% vs 36.91%). If we fur-\nther zoom in on the retrieval performance on non-\ncomparison questions for which ﬁnding the two\nentities involved is less trivial, we can see that the\nrecall on d2 improves from 27.88% to 53.23%, al-\nmost doubling the number of questions we have",
        "g the off-the-shelf 3D segmentor and then\nconvert the task into a 3D object selection task. Without\nrelying on the constructed point cloud, our method could\ndirectly decode the accurate 3D bounding boxes from 3D\npatches and achieve the SOTA performance (49.8 Acc@0.25\non Multi3DRefer) in the single-stage manner.\n5.4. Evaluation on 2D benchmarks\nSince our model is trained on the joint 2D and 3D datasets,\nwe evaluate it on the 2D video benchmarks to ensure it"
      ],
      "execution_time": 0.5145330429077148,
      "category": "comparison",
      "query_id": 35,
      "difficulty": "hard"
    },
    {
      "question": "How can we improve retrieval quality in RAG?",
      "answer": "ctual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine Similarity\nARES‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\nAccuracy\nAccuracy\nAccuracy\nTruLens‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\n*\nCRUD†\nRetrieval Quality\n\nctual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine Similarity\nARES‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\nAccuracy\nAccuracy\nAccuracy\nTruLens‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\n*\nCRUD†\nRetrieval Quality\n\ntegorized as most challenging until they met the Llama-based “model-as-judge” criteria. By refining\nthese challenging problems, the coding data achieves a balance between quality and difficulty, resulting in\noptimal downstream performance.\n4.3.2\nMultilinguality\nWe describe how we improve Llama 3’s multilingual capabilities, including training an expert specialized on\nsubstantially more multilingual data, sourcing and generating high quality multilingual instruction tuning",
      "relevant_chunks": 3,
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "2407.21783.pdf"
      ],
      "scores": [
        0.30230034493741287,
        0.30230034493741287,
        0.20075064135470597
      ],
      "contexts": [
        "ctual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine Similarity\nARES‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\nAccuracy\nAccuracy\nAccuracy\nTruLens‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\n*\nCRUD†\nRetrieval Quality",
        "ctual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL†\nGeneration Quality\nCounterfactual Robustness\nR-Rate (Reappearance Rate)\nRAGAS‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine Similarity\nARES‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\nAccuracy\nAccuracy\nAccuracy\nTruLens‡\nRetrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\n*\nCRUD†\nRetrieval Quality",
        "tegorized as most challenging until they met the Llama-based “model-as-judge” criteria. By refining\nthese challenging problems, the coding data achieves a balance between quality and difficulty, resulting in\noptimal downstream performance.\n4.3.2\nMultilinguality\nWe describe how we improve Llama 3’s multilingual capabilities, including training an expert specialized on\nsubstantially more multilingual data, sourcing and generating high quality multilingual instruction tuning"
      ],
      "execution_time": 0.5288105010986328,
      "category": "optimization",
      "query_id": 36,
      "difficulty": "medium"
    },
    {
      "question": "What techniques reduce hallucinations in RAG systems?",
      "answer": "Ms\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present considerable complexities, manifesting\nas outputs that are either factually inaccurate or misleading. These hallucinations occur when\nthe content generated by the LLM does not align with real-world facts, fails to accurately reflect\nthe user’s query, or is not supported by the retrieved information. Such hallucinations can stem\n\nLM\nhallucinations in depth (§3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (§6) and delineate potential pathways for forthcoming research (§7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a\n\nfor machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?",
      "relevant_chunks": 3,
      "sources": [
        "2311.05232.pdf",
        "2311.05232.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.2430693120277332,
        0.22931778242605805,
        0.2163009093906852
      ],
      "contexts": [
        "Ms\n[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still\nproduce hallucinations [16]. Hallucinations in RAG present considerable complexities, manifesting\nas outputs that are either factually inaccurate or misleading. These hallucinations occur when\nthe content generated by the LLM does not align with real-world facts, fails to accurately reflect\nthe user’s query, or is not supported by the retrieved information. Such hallucinations can stem",
        "LM\nhallucinations in depth (§3), followed by a review of various strategies and benchmarks employed\nfor the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches\ndesigned to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by\ncurrent RAG systems (§6) and delineate potential pathways for forthcoming research (§7).\n2\nDEFINITIONS\nFor the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a",
        "for machine learning?\nWhat is the XGBoost Machine Learning algorithm?\nWhat is the Kaggle platform?\nWhat is the company OpenAI?\nE.6.22\nManagement\nWhat can you tell me about Bob Iger’s leadership at The Walt Disney Company?\nWhat can you tell me about Jamie Dimon’s leadership at JPMorgan Chase?\nWho is Larry Page and what is his relation to Google?\nWho is Sheryl Sandberg and what is her role at Facebook?\nWho is Reed Hastings and what was his role at Netflix?"
      ],
      "execution_time": 0.5494880676269531,
      "category": "optimization",
      "query_id": 37,
      "difficulty": "hard"
    },
    {
      "question": "How can we optimize RAG for low-latency applications?",
      "answer": "efront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG.\n\nefront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG.\n\ny target prediction, we employ a delayed pattern approach (Kharitonov et al., 2021), which enhances\nthe model’s ability to capture and reproduce long-range prosodic dependencies. This approach contributes to\nthe naturalness and expressiveness of the synthesized speech, ensuring low-latency and high-quality output.\n8.4\nSpeech Understanding Results\nWe evaluate the speech understanding capabilities of our speech interface for Llama 3 on three tasks: (1)",
      "relevant_chunks": 3,
      "sources": [
        "2312.10997.pdf",
        "RAG_Survey_2023.pdf",
        "2407.21783.pdf"
      ],
      "scores": [
        0.1435078598964129,
        0.1435078598964129,
        0.12852248470347324
      ],
      "contexts": [
        "efront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG.",
        "efront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding\ntraditional\nsingle-hop/multi-hop\nQA,\nmultiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG.",
        "y target prediction, we employ a delayed pattern approach (Kharitonov et al., 2021), which enhances\nthe model’s ability to capture and reproduce long-range prosodic dependencies. This approach contributes to\nthe naturalness and expressiveness of the synthesized speech, ensuring low-latency and high-quality output.\n8.4\nSpeech Understanding Results\nWe evaluate the speech understanding capabilities of our speech interface for Llama 3 on three tasks: (1)"
      ],
      "execution_time": 0.5480756759643555,
      "category": "optimization",
      "query_id": 38,
      "difficulty": "hard"
    },
    {
      "question": "What is context compression in RAG and why is it useful?",
      "answer": "e \nwhy questions and the retrieved passages. \nTo evaluate EWAQ, test set of 250 why questions with \ntheir correct answers answered manually is used. Since there \nare no Arabic QA systems implemented the why questions \nsystems, search engines are used to compare the accuracy \nEWAQ with them. \nUsing \nentailment \nbased \nsimilarity \nwith \nsome \nmodification suited to Arabic language improves the \naccuracy of the why QA system. The overall accuracy with\n\nis the La Ferrassie Cave?\nWhat is the Dolmen of Menga?\nWhat is the Bronze Age Nuragic civilization?\nWhat is the Chauvet-Pont-d’Arc Cave?\nWhat is the Altamira Cave?\n63\nE.6.32\nPsychology\nWhat is the aim of The Minnesota Twin Study in the context of nature vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?\n\nlexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco.",
      "relevant_chunks": 3,
      "sources": [
        "arxiv_1907.04149_2019.pdf",
        "2403.18802.pdf",
        "2403.18802.pdf"
      ],
      "scores": [
        0.27367916945612314,
        0.2341506730653444,
        0.2250597265577663
      ],
      "contexts": [
        "e \nwhy questions and the retrieved passages. \nTo evaluate EWAQ, test set of 250 why questions with \ntheir correct answers answered manually is used. Since there \nare no Arabic QA systems implemented the why questions \nsystems, search engines are used to compare the accuracy \nEWAQ with them. \nUsing \nentailment \nbased \nsimilarity \nwith \nsome \nmodification suited to Arabic language improves the \naccuracy of the why QA system. The overall accuracy with",
        "is the La Ferrassie Cave?\nWhat is the Dolmen of Menga?\nWhat is the Bronze Age Nuragic civilization?\nWhat is the Chauvet-Pont-d’Arc Cave?\nWhat is the Altamira Cave?\n63\nE.6.32\nPsychology\nWhat is the aim of The Minnesota Twin Study in the context of nature vs nurture debate?\nWhat is the goal of Project Implicit, an online research project on implicit social cognition?\nWhat is the Asch Conformity Experiments, and how does it illustrate the influence of group pressure\non individual judgment?",
        "lexity?\nWho is Raymond Kurzweil?\nWhat is the Y2K bug?\nWhat is TensorFlow machine learning platform?\nWhat is the Raspberry Pi Zero?\nE.6.11\nComputer Security\nWho is the cybersecurity analyst Kevin Mitnick?\nWhat is the firewall FortiGate by Fortinet?\nWhat is the vulnerability assessment tool Nessus?\nWhat is the software Norton Antivirus?\nWhat is the password manager LastPass?\nE.6.12\nEconomics\nGive me an overview of the Federal Reserve Bank of San Francisco."
      ],
      "execution_time": 0.5357351303100586,
      "category": "optimization",
      "query_id": 39,
      "difficulty": "hard"
    },
    {
      "question": "How does prompt engineering affect RAG quality?",
      "answer": "to shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-\n\nto the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using\n\nr relationship. Both kinds of element\ninstance are output in a single list of delimited tuples.\nA.2\nSelf-Reflection\nThe choice of prompt engineering techniques has a strong impact on the quality of knowledge graph\nextraction (Zhu et al., 2024), and different techniques have different costs in terms of tokens con-\nsumed and generated by the model. Self-reflection is a prompt engineering technique where the",
      "relevant_chunks": 3,
      "sources": [
        "2401.05856.pdf",
        "2401.05856.pdf",
        "2404.16130.pdf"
      ],
      "scores": [
        0.2324843777993875,
        0.22772078983398783,
        0.19260441052471589
      ],
      "contexts": [
        "to shine a light on what issues\nengineers will face and what software engineering research is nec-\nessary to realise solutions with the current state-of-the-art RAG\nsystems.\nEmerging work has looked at benchmarking RAG systems [3]\nbut not at the failures occurring during implementation. Software\nengineering research has investigated the use of RAG systems for\ncode-related tasks [15]. However, the application of RAG systems\nis broader than software engineering tasks. This paper comple-",
        "to the challenges with creating robust\nRAG systems. As advances in LLMs continue to take place, the\nsoftware engineering community has a responsibility to provide\nknowledge on how to realise robust systems with LLMs. This work\nis an important step for robustness in building RAG systems.\nResearch questions for this work include:\n• What are the failure points that occur when engineering a RAG\nsystem? (section 5) We present an empirical experiment using",
        "r relationship. Both kinds of element\ninstance are output in a single list of delimited tuples.\nA.2\nSelf-Reflection\nThe choice of prompt engineering techniques has a strong impact on the quality of knowledge graph\nextraction (Zhu et al., 2024), and different techniques have different costs in terms of tokens con-\nsumed and generated by the model. Self-reflection is a prompt engineering technique where the"
      ],
      "execution_time": 0.5388302803039551,
      "category": "optimization",
      "query_id": 40,
      "difficulty": "medium"
    }
  ],
  "metrics": {
    "average_execution_time": 0.5272114276885986,
    "average_top_score": 0.24398713506558012,
    "total_queries": 50
  }
}